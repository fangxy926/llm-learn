{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ee2d40-1aa3-499b-b32d-95ba0771d98b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 💡 这节课会带给你\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7348e5-9690-4390-bed8-939cd2a6916e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 如何用你的垂域数据补充 LLM 的能力\n",
    "1. 如何构建你的垂域（向量）知识库\n",
    "1. 搭建一套完整 RAG 系统需要哪些模块\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391302b1-a2cf-4e76-b0d2-844ed587c677",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一、什么是检索增强的生成模型（RAG）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5936819-e086-439c-822e-72ba4478c814",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1、LLM 固有的局限性\n",
    "\n",
    "1. LLM 的知识不是实时的\n",
    "2. LLM 可能不知道你私有的领域/业务知识\n",
    "\n",
    "<img src=\"gpt-llama2.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6a2f9-b5a2-40f0-8b1d-a2c151e9529b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2、检索增强生成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5130fe2-df4b-4211-be1f-11bf688df9d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RAG（Retrieval Augmented Generation）顾名思义，通过**检索**的方法来增强**生成模型**的能力。\n",
    "\n",
    "<video src=\"RAG.mp4\" controls=\"controls\" width=800px style=\"margin-left: 0px\"></video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc0fbd-86b9-4fdc-b8f0-b6b0b1238775",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>类比：</b>你可以把这个过程想象成开卷考试。让 LLM 先翻书，再回答问题。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a310a91-d279-4c96-ad0f-648d67bb0e90",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 二、就在上周 OpenAI Assistant API 内置了这个能力\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359fa3d-1e1f-4d3c-969f-9539e6bfbe61",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "from openai import OpenAI # 需要1.2以上版本\n",
    "import os\n",
    "# 加载环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地 .env 文件，里面定义了 OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\")\n",
    ")\n",
    "\n",
    "# 上传文件\n",
    "file = client.files.create(\n",
    "  file=open(\"llama2.pdf\", \"rb\"),\n",
    "  purpose='assistants'\n",
    ")\n",
    "\n",
    "# 创建 Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  instructions=\"你是个问答机器人，你根据给定的知识回答用户问题。\",\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  tools=[{\"type\": \"retrieval\"}],\n",
    "  file_ids=[file.id]\n",
    ")\n",
    "\n",
    "# 创建 Thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# 创建 User Message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Llama 2有多少参数\"\n",
    ")\n",
    "\n",
    "# 创建 Run 实例，同时给 Assistant 提供指令\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"请用中文回答用户的问题。\",\n",
    ")\n",
    "\n",
    "# 等待 Run 完成\n",
    "while run.status != \"completed\":\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "      thread_id=thread.id,\n",
    "      run_id=run.id\n",
    "    )\n",
    "\n",
    "# 获取 Run 的结果\n",
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "for turn in reversed(messages.data):\n",
    "    print(f\"{turn.role.upper()}: \"+turn.content[0].text.value)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dac0fd-4634-4f79-9465-8f8cd160b338",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"assistant_api_retrieval.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e1578-7578-4c58-8d07-e51f49e4acb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>我们为什么仍然需要了解整个实现过程？</b>\n",
    "<ol>\n",
    "<li>如果不能使用 OpenAI，还是需要手工实现 RAG 流程</li>\n",
    "<li>了解 RAG 的原理，可以指导你的产品开发（回忆 GitHub Copilot）</li>\n",
    "<li>用私有知识增强 LLM 的能力，是一个通用的方法论</li>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22db5c8-983b-43c1-a6cf-eff0add77c21",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 三、RAG 系统的基本搭建流程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca65e75-2435-473a-8698-2937746c6555",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "先看效果：http://localhost:8888/\n",
    "\n",
    "搭建过程：\n",
    "\n",
    "1. 文档加载，并按一定条件**切割**成片段\n",
    "2. 将切割的文本片段灌入**检索引擎**\n",
    "3. 封装**检索接口**\n",
    "4. 构建**调用流程**：Query -> 检索 -> Prompt -> LLM -> 回复\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43fd17-0a47-45da-99f5-514f0c11f66d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1、文档的加载与切割\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5accc5c2-0bd4-4f6a-aad3-9e359cd01fa3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Using cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from pdfminer.six) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.11/site-packages (from pdfminer.six) (41.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20221105\n"
     ]
    }
   ],
   "source": [
    "# 安装 pdf 解析库\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab7e0c0-8328-437a-84b5-56edbeba00bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a8065c-a11e-4645-baf0-cca80cd07aaa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    '''从 PDF 文件中（按指定页码）提取文字'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # 如果指定了页码范围，跳过范围外的页\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59639eba-5107-4270-8066-ea740005e294",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\", min_line_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10c58a2-9171-47d2-b1e6-672dccf9036a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "\n",
      " Hugo Touvron∗ Louis Martin† Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗\n",
      "\n",
      " GenAI, Meta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in paragraphs[:3]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f04b2-acbe-405d-8269-e0f3488b1a1e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2、检索引擎\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8788e99-fd6d-48e0-bf46-9601c4bf89c5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "先看一个最基础的实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016d33c5-b6b6-4167-a3ce-db904b80edd4",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch7\n",
      "  Using cached elasticsearch7-7.17.9-py2.py3-none-any.whl (386 kB)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from elasticsearch7) (1.26.18)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from elasticsearch7) (2023.5.7)\n",
      "Installing collected packages: elasticsearch7\n",
      "Successfully installed elasticsearch7-7.17.9\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "# 安装 ES 客户端\n",
    "!pip install elasticsearch7\n",
    "# 安装NLTK（文本处理方法库）\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0bd3a9-0bc4-4910-abe0-41b1ee9eb648",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")  # 屏蔽 ES 的一些Warnings\n",
    "\n",
    "nltk.download('punkt')  # 英文切词、词根、切句等方法\n",
    "nltk.download('stopwords')  # 英文停用词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e4848e3-f531-4b37-ab80-6b632066da2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_keywords(input_string):\n",
    "    '''（英文）文本只保留关键字'''\n",
    "    # 使用正则表达式替换所有非字母数字的字符为空格\n",
    "    no_symbols = re.sub(r'[^a-zA-Z0-9\\s]', ' ', input_string)\n",
    "    word_tokens = word_tokenize(no_symbols)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    # 去停用词，取词根\n",
    "    filtered_sentence = [ps.stem(w)\n",
    "                         for w in word_tokens if not w.lower() in stop_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22396b7e-9c43-45d5-be47-7db91f67f659",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "将文本灌入检索引擎\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccda62e1-fc0e-4a4d-ba29-f55b0475c9cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 创建Elasticsearch连接\n",
    "es = Elasticsearch(\n",
    "    hosts=['http://117.50.198.53:9200'],  # 服务地址与端口\n",
    "    http_auth=(\"elastic\", \"FKaB1Jpz0Rlw0l6G\"),  # 用户名，密码\n",
    ")\n",
    "\n",
    "# 2. 定义索引名称\n",
    "index_name = \"string_index\"\n",
    "\n",
    "# 3. 如果索引已存在，删除它（仅供演示，实际应用时不需要这步）\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# 4. 创建索引\n",
    "es.indices.create(index=index_name)\n",
    "\n",
    "# 5. 灌库指令\n",
    "actions = [\n",
    "    {\n",
    "        \"_index\": index_name,\n",
    "        \"_source\": {\n",
    "            \"keywords\": to_keywords(para),\n",
    "            \"text\": para\n",
    "        }\n",
    "    }\n",
    "    for para in paragraphs\n",
    "]\n",
    "\n",
    "# 6. 文本灌库\n",
    "helpers.bulk(es, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d3914-536c-43f3-b78c-81fe4cc46e9f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "实现关键字检索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f645f243-a8f6-44b7-a8ce-de6ba8b8ec92",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def search(query_string, top_n=3):\n",
    "    # ES 的查询语言\n",
    "    search_query = {\n",
    "        \"match\": {\n",
    "            \"keywords\": to_keywords(query_string)\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, query=search_query, size=top_n)\n",
    "    return [hit[\"_source\"][\"text\"] for hit in res[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3ee897-fd31-4190-a3fb-6759e3c72ab4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = search(\"how many parameters does llama 2 have?\", 2)\n",
    "for r in results:\n",
    "    print(r+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffd78a-a51a-42fc-8bf7-1360411b0661",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3、LLM 接口封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07682fcf-d945-4bac-aec1-a1db6ea9ef03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "# 加载环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地 .env 文件，里面定义了 OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65b9fa18-b3ba-430a-bbc2-0a6438b536fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    '''封装 openai 接口'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # 模型输出的随机性，0 表示随机性最小\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff69c31-f069-424f-8484-396edf32ce11",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4、Prompt 模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156f0795-0b70-42e2-91a9-a0ac4a0521c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    '''将 Prompt 模板赋值'''\n",
    "    prompt = prompt_template\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, str):\n",
    "            val = v\n",
    "        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = '\\n'.join(v)\n",
    "        else:\n",
    "            val = str(v)\n",
    "        prompt = prompt.replace(f\"__{k.upper()}__\", val)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5046043b-ce52-4dcd-862e-824f1d21db1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "\n",
    "已知信息:\n",
    "__INFO__\n",
    "\n",
    "用户问：\n",
    "__QUERY__\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d15d0-2fce-41e3-a233-b95712c37db4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.5、RAG Pipeline 初探\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3178c27-4d40-4763-ab7e-9ce22d9897aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Prompt===\n",
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "确保你的回复完全依据下述已知信息。不要编造答案。\n",
      "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "\n",
      "已知信息:\n",
      " Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      "用户问：\n",
      "how many parameters does llama 2 have?\n",
      "\n",
      "请用中文回答用户问题。\n",
      "\n",
      "===回复===\n",
      "Llama 2有7B、13B和70B三种参数大小的变体。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how many parameters does llama 2 have?\"\n",
    "\n",
    "# 1. 检索\n",
    "search_results = search(user_query, 2)\n",
    "\n",
    "# 2. 构建 Prompt\n",
    "prompt = build_prompt(prompt_template, info=search_results, query=user_query)\n",
    "print(\"===Prompt===\")\n",
    "print(prompt)\n",
    "\n",
    "# 3. 调用 LLM\n",
    "response = get_completion(prompt)\n",
    "# response = get_completion_ernie(prompt)\n",
    "print(\"===回复===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38461bf-11bd-48fe-aba9-07cb43a98f4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>扩展阅读：</b>\n",
    "<ol>\n",
    "<ul>Elasticsearch（简称ES）是一个广泛应用的开源搜索引擎: https://www.elastic.co/</ul>\n",
    "<ul>关于ES的安装、部署等知识，网上可以找到大量资料，例如: https://juejin.cn/post/7104875268166123528</ul>\n",
    "<ul>关于经典信息检索技术的更多细节，可以参考: https://nlp.stanford.edu/IR-book/information-retrieval-book.html</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58d517-0738-460c-9b56-8937db1a3874",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.6、关键字检索的局限性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5997a5-7955-4a02-bea1-084e7231ed6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同一个语义，用词不同，可能导致检索不到有效的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40d6fe7a-cfeb-4dd6-b75e-9a998928650e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      " In Figure 18, we report the violation percentage on single- and multi-turn conversations, respectively. A trend across models is that multi-turn conversations are more prone to inducing unsafe responses. That said, Llama 2-Chat still performs well compared to baselines, especially on multi-turn conversations. We also observe that Falcon performs particularly well on single-turn conversations (largely due to its conciseness) but much worse on multi-turn conversations, which could be due to its lack of multi-turn supervised fine-tuning data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user_query=\"Does llama 2 have a chat version?\"\n",
    "user_query = \"Does llama 2 have a conversational variant?\"\n",
    "\n",
    "search_results = search(user_query, 2)\n",
    "\n",
    "for res in search_results:\n",
    "    print(res+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b8b7c-09dc-4e9c-be2c-40cc1eadea5e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 四、向量检索\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a930d-dcb1-46f6-8f69-f0a4a62663be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1、文本向量（Text Embeddings）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91aa9d-e112-4200-9e3d-7c14826c668d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 将文本转成一组浮点数：每个下标 $i$，对应一个维度\n",
    "2. 整个数组对应一个 $n$ 维空间的一个点，即**文本向量**又叫 Embeddings\n",
    "3. 向量之间可以计算距离，距离远近对应**语义相似度**大小\n",
    "\n",
    "<br />\n",
    "<img src=\"embeddings.png\" style=\"margin-left: 0px\" width=800px>\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84fb43-b02f-4089-9eda-d02f90359915",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.1、文本向量是怎么得到的（选）\n",
    "\n",
    "1. 构建相关（正立）与不相关（负例）的句子对儿样本\n",
    "2. 训练双塔式模型，让正例间的距离小，负例间的距离大\n",
    "\n",
    "例如：\n",
    "\n",
    "<img src=\"sbert.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e61d11-40f6-4fe5-81b3-16bfbb8bbbe5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>扩展阅读：https://www.sbert.net</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2153c2-f418-41e5-9aad-0a52a2907579",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2、向量间的相似度计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4cc12-baf8-4bef-b1c7-aa584dedf0f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"sim.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bf10b44-8d90-481a-9d71-b47a61d377a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49c5c129-48ca-43c0-826c-a0f6febded89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    '''余弦距离 -- 越大越相似'''\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "\n",
    "def l2(a, b):\n",
    "    '''欧式距离 -- 越小越相似'''\n",
    "    x = np.asarray(a)-np.asarray(b)\n",
    "    return norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65986a2b-0b7e-44e9-a0e5-e6204250b69b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    '''封装 OpenAI 的 Embedding 模型接口'''\n",
    "    data = client.embeddings.create(input=texts, model=model).data\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "135ab2d9-7709-41c9-90d9-c20f78d402ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0072620222344994545, -0.006227712146937847, -0.010517913848161697, 0.001511403825134039, -0.010678159072995186, 0.029252037405967712, -0.019783001393079758, 0.0053937085904181, -0.017029697075486183, -0.01215678546577692]\n"
     ]
    }
   ],
   "source": [
    "test_query = [\"测试文本\"]\n",
    "vec = get_embeddings(test_query)[0]\n",
    "print(vec[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae25ea97-4ddc-4a6d-8b30-2beaa1381775",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "1.0\n",
      "0.8223882670677338\n",
      "0.8299506820996052\n",
      "0.7982156472341975\n",
      "0.7670777912968042\n",
      "0.7934906412584019\n",
      "\n",
      "Euclidean distance:\n",
      "0.0\n",
      "0.596006294791316\n",
      "0.5831797759221408\n",
      "0.6352705975574178\n",
      "0.6825279906988642\n",
      "0.642665330225979\n"
     ]
    }
   ],
   "source": [
    "query = \"国际争端\"\n",
    "\n",
    "# 且能支持跨语言\n",
    "# query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query])[0]\n",
    "doc_vecs = get_embeddings(documents)\n",
    "\n",
    "print(\"Cosine distance:\")\n",
    "print(cos_sim(query_vec, query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))\n",
    "\n",
    "print(\"\\nEuclidean distance:\")\n",
    "print(l2(query_vec, query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd9cf4-847b-4c3a-b297-2a23add05c93",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3、向量数据库\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c562b-0eac-4fcc-9d5c-da7903fdb310",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "向量数据库，是专门为向量检索设计的中间件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b17272d-72a3-4778-a403-f489b4c0d900",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /opt/conda/lib/python3.11/site-packages (0.4.17)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.10.13)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /opt/conda/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.11/site-packages (from chromadb) (0.104.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.11/site-packages (from chromadb) (0.24.0.post1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (4.8.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.16.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.11/site-packages (from chromadb) (0.15.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.11/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.11/site-packages (from chromadb) (6.0.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.59.3)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (28.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.11/site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.11/site-packages (from chromadb) (6.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.11/site-packages (from chromadb) (1.26.2)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.23.4)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.6.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.21.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.42b0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af94f142-43b7-41ce-b89c-906a66e14386",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 为了演示方便，我们只取两页（第一章）\n",
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\", page_numbers=[\n",
    "                                   2, 3], min_line_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d20de15f-948a-468b-a997-53e1b167dbe1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # 为了演示，实际不需要每次 reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # 创建一个 collection\n",
    "        self.collection = chroma_client.get_or_create_collection(name=\"demo\")\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents, metadata={}):\n",
    "        '''向 collection 中添加文档与向量'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # 每个文档的向量\n",
    "            documents=documents,  # 文档的原文\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # 每个文档的 id\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''检索向量数据库'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e941d71-4c4e-48a1-872b-9ff73f4f71fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5198c009-12fd-4853-8692-a758132fa7d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_query = \"Llama 2有多少参数\"\n",
    "\n",
    "results = vector_db.search(user_query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4a782ca-f992-437d-99fc-27102d2fe7c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      " In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in results['documents'][0]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9303e-bad3-45e4-8f20-5cf3411a993b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3.1、向量数据库服务\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b97ebf-5b8f-462f-8af6-65073d59db4f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Server 端\n",
    "\n",
    "```sh\n",
    "chroma run --path /db_path\n",
    "```\n",
    "\n",
    "Client 端\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf79292-4e24-4fc0-9dc5-3efb7b818171",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3.2、主流向量数据库功能对比\n",
    "\n",
    "<img src=\"vectordb.png\" style=\"margin-left: 0px\" width=800px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6f297-3b57-4293-9d06-67619794a3ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- FAISS: Meta 开源的向量检索引擎 https://github.com/facebookresearch/faiss\n",
    "- Pinecone: 商用向量数据库，只有云服务 https://www.pinecone.io/\n",
    "- Milvus: 开源向量数据库，同时有云服务 https://milvus.io/\n",
    "- Weaviate: 开源向量数据库，同时有云服务 https://weaviate.io/\n",
    "- Qdrant: 开源向量数据库，同时有云服务 https://qdrant.tech/\n",
    "- PGVector: Postgres 的开源向量检索引擎 https://github.com/pgvector/pgvector\n",
    "- RediSearch: Redis 的开源向量检索引擎 https://github.com/RediSearch/RediSearch\n",
    "- ElasticSearch 也支持向量检索 https://www.elastic.co/enterprise-search/vector-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5d505-84ac-4846-8696-9f316d7ca1e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.4、基于向量检索的 RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c381a46-b8e7-4a4a-9fe1-669a7dcbc687",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. 构建 Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, info=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. 调用 LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "494ed1f8-b194-4d9d-a3fa-fbffda1e4d11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2有7B、13B和70B参数的变体。\n"
     ]
    }
   ],
   "source": [
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")\n",
    "\n",
    "user_query = \"llama 2有多少参数？\"\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb8f84-bc9b-4f45-a75e-db9b563acb79",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.5、如果想要换个模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48f7bdf-3297-492d-b066-ce1b9687539c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# 通过鉴权接口获取 access token\n",
    "\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    使用 AK，SK 生成鉴权签名（Access Token）\n",
    "    :return: access_token，或是None(如果错误)\n",
    "    \"\"\"\n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token\"\n",
    "    params = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": os.getenv('ERNIE_CLIENT_ID'),\n",
    "        \"client_secret\": os.getenv('ERNIE_CLIENT_SECRET')\n",
    "    }\n",
    "\n",
    "    return str(requests.post(url, params=params).json().get(\"access_token\"))\n",
    "\n",
    "# 调用文心千帆 调用 BGE Embedding 接口\n",
    "\n",
    "\n",
    "def get_embeddings_bge(prompts):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/bge_large_en?access_token=\" + get_access_token()\n",
    "    payload = json.dumps({\n",
    "        \"input\": prompts\n",
    "    })\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", url, headers=headers, data=payload).json()\n",
    "    data = response[\"data\"]\n",
    "    return [x[\"embedding\"] for x in data]\n",
    "\n",
    "\n",
    "# 调用文心4.0对话接口\n",
    "def get_completion_ernie(prompt):\n",
    "\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions_pro?access_token=\" + get_access_token()\n",
    "    payload = json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", url, headers=headers, data=payload).json()\n",
    "\n",
    "    return response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2791b8a-14de-4cd6-a51b-650a27f2d537",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "new_vector_db = MyVectorDBConnector(\n",
    "    \"demo_ernie\",\n",
    "    embedding_fn=get_embeddings_bge\n",
    ")\n",
    "# 向向量数据库中添加文档\n",
    "new_vector_db.add_documents(paragraphs)\n",
    "# 创建一个RAG机器人\n",
    "new_bot = RAG_Bot(\n",
    "    new_vector_db,\n",
    "    llm_api=get_completion_ernie\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f81a9dc1-5347-481c-90ad-61978498e65f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2具有7B、13B和70B参数的不同变体。因此，Llama 2具有多种参数数量，具体取决于所选的变体。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how many parameters does llama 2 have?\"\n",
    "\n",
    "response = new_bot.chat(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3041c5b-7d29-4264-8dd5-f553a7342237",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 五、实战 RAG 系统的进阶知识\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b07741-a60e-4927-b2f1-f037d6fce69c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.1、文本分割的粒度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6cfe1d-980e-455b-9ec4-be7dd8670041",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**缺陷**\n",
    "\n",
    "1. 粒度太大可能导致检索不精准，粒度太小可能导致信息不全面\n",
    "2. 问题的答案可能跨越两个片段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c9dcbae-2b30-4c7c-91eb-6e4b223f82dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(paragraphs)\n",
    "\n",
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46af635a-4605-4991-81c3-e50eeb6efb1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      "====回复====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根据已知信息，我们相信在安全的情况下，公开发布LLM（Llama 2）将对社会产生净利益。然而，LLM是一项新技术，使用中存在潜在风险。目前的测试仅涵盖了英语，并不能覆盖所有情况。因此，在部署LLM应用之前，开发人员应根据其具体应用对模型进行安全测试和调整。我们提供了负责任的使用指南和代码示例，以促进LLM的安全部署。根据已知信息，我们无法确定LLM 2是否可以商用。'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"llama 2可以商用吗？\"\n",
    "# user_query=\"llama 2 chat有多少参数\"\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "print(\"====回复====\")\n",
    "bot.chat(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2797d4-1496-42b4-a121-ea8661a25459",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for p in paragraphs:\n",
    "    print(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5076f-a277-45f1-a094-30c6a0b1d84d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**改进**: 按一定粒度，部分重叠式的切割文本，使上下文更完整\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42549909-8516-4ca1-8bae-75ec31aff20d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''按指定 chunk_size 和 overlap_size 交叠割文本'''\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # 向前计算重叠部分\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # 向后计算当前chunk\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2de4f8de-1a31-46b7-ab9b-1d20ebb8fc3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunks = split_text(paragraphs, 300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ece25b9-126a-4cc0-a74a-3eda2dc24375",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(chunks)\n",
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "365c7c37-44f3-4774-8bd6-be0482322119",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n",
      "Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society.\n",
      "\n",
      "====回复====\n",
      "是的，Llama 2可以商用。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"llama 2可以商用吗？\"\n",
    "# user_query=\"llama 2 chat有多少参数\"\n",
    "\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====回复====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4d28b-392f-4bec-8134-1f52659048be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.2、检索后排序（选）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd58f1-cbd3-4a9f-a4f2-d8592b152d0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**问题**: 有时，最合适的答案不一定排在检索的最前面\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81cc6d27-a7c3-49e2-aefa-4a5c39db0d2d",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.\n",
      "\n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial use‡: 1.\n",
      "\n",
      "We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      "====回复====\n",
      "根据已知信息，我们相信在安全的情况下，公开发布LLMs将对社会产生净利益。像所有的LLMs一样，Llama 2是一项新技术，在使用过程中存在潜在风险。然而，我们也分享了在开发Llama 2和Llama 2-Chat过程中所做的新观察，例如工具使用的出现和知识的时间组织。根据已知信息，我们无法直接回答Llama 2的安全性问题。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "search_results = vector_db.search(user_query, 5)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====回复====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf8d40-6b68-42a5-a1d8-c7aece1cd876",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**方案**:\n",
    "\n",
    "1. 检索时过招回一部分文本\n",
    "2. 通过一个排序模型对 query 和 document 重新打分排序\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f9477-1fad-4269-a73a-b2cbe475500f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"sbert-rerank.png\" style=\"margin-left: 0px\" width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0343d0-9337-4b97-90b0-c79a8e226ef2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "以下代码不要在服务器上运行，会死机！可下载左侧 rank.py 在自己本地运行。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147d24c-88ab-4bc9-80d6-be6eb3afa185",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33253613-76d3-4229-a4af-6dd44a386e8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f55986d0-fd7b-42b9-a786-fbc1f8550c1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.470587253570557\tWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "5.383452415466309\tIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "4.709953308105469\tWe provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      "4.543964862823486\tWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.\n",
      "\n",
      "4.033888816833496\tAdditionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial use‡: 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "\n",
    "scores = model.predict([(user_query, doc)\n",
    "                       for doc in search_results['documents'][0]])\n",
    "# 按得分排序\n",
    "sorted_list = sorted(\n",
    "    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)\n",
    "for score, doc in sorted_list:\n",
    "    print(f\"{score}\\t{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88431f70-dab3-4e5d-a67e-e50e26b7f38a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 六、向量模型的本地部署\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd662d-0728-4b7c-9e92-875ef4fcdb8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "以下代码不要在服务器上运行，会死机！可下载左侧 bge.py 在自己本地运行。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d6fbbd5-3948-456a-b102-33ad228d0732",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-large-zh-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36f9599e-8c0a-4041-8d4a-5efee07ca1d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "1.0\n",
      "0.47276446\n",
      "0.38867018\n",
      "0.32856295\n",
      "0.31619197\n",
      "0.3093862\n"
     ]
    }
   ],
   "source": [
    "query = \"国际争端\"\n",
    "\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = model.encode(query, normalize_embeddings=True)\n",
    "doc_vecs = [\n",
    "    model.encode(doc, normalize_embeddings=True)\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "print(\"Cosine distance:\")  # 越大越相似\n",
    "print(cos_sim(query_vec, query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d7e78-d56c-4d7e-82f8-874ce8a85df1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>更多模型：https://github.com/FlagOpen/FlagEmbedding</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d73561-5b33-47b6-8883-d546becad59b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "    <ol>\n",
    "        <li>不是每个 Embedding 模型都对余弦距离和欧氏距离同时有效</li>\n",
    "        <li>哪种相似度计算有效要阅读模型的说明（通常都支持余弦距离计算）</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a99e3-d692-404a-81cf-6f67543a1f40",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 总结\n",
    "\n",
    "### RAG 的流程\n",
    "\n",
    "1. 离线步骤：文档加载->切分->向量化->灌库\n",
    "2. 在线步骤：问题->向量化->检索->Prompt->LLM->回复\n",
    "\n",
    "### 我用了一个开源的 RAG，不好使怎么办？\n",
    "\n",
    "1. 检查预处理效果：文档加载是否正确，切割的是否合理\n",
    "2. 测试检索效果：问题检索回来的文本片段是否包含答案\n",
    "3. 测试大模型能力：给定问题和包含答案文本片段的前提下，大模型能不能正确回答问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f5fdd-91a9-4a24-83c4-1f10f3f261fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 作业\n",
    "\n",
    "做个自己的 ChatPDF。需求：\n",
    "\n",
    "1. 从本地加载 PDF 文件，基于 PDF 的内容对话\n",
    "2. 可以无前端，只要能在命令行运行就行\n",
    "3. 其它随意发挥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}