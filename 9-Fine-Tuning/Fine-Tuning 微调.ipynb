{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbee847",
   "metadata": {},
   "source": [
    "## 模型的本质\n",
    "\n",
    "通俗（不严谨）的讲：**模型**是包含输入、输出的函数表达式  $y=F(x;\\omega)$\n",
    "\n",
    "- 输入$x$：可以是一个词、一个句子、一篇文章或图片、语音、视频 ... 大模型中输入都被表示成一个矩阵（专业名词叫**张量**，tensor）\n",
    "  \n",
    "- 输出$y$：可以是「是否」（{0,1}）、标签（{0,1,2,3...}）、一个数值（回归问题）、下一个词的概率 ...\n",
    "\n",
    "- 表达式$F$ 就是神经网络结构（这里特指深度学习）\n",
    "\n",
    "- 它有一组**参数** $\\omega$，这就是我们要训练的部分\n",
    "\n",
    "\n",
    "\n",
    "通俗的讲：**训练模型**，就是确定这组参数的取值\n",
    "\n",
    "- 用数学（数值分析）方法找到使模型在训练集上表现足够好的一个值</li>\n",
    "- 表现足够好，就是说，对每个数据样本$(x,y)$，使 $F(x;\\omega)$ 的值尽可能接近 $y$</li>\n",
    "\n",
    "所谓的“大模型”，“大”在训练模型的参数非常多，多达7B（70亿）、72B（720亿）...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f35b6d",
   "metadata": {},
   "source": [
    "## 模型训练 vs 预训练 vs 微调\n",
    "\n",
    "\n",
    "### 1、训练\n",
    "\n",
    "利用给定的数据集，**从无到有**地训练出一个模型；我们希望找到一组参数$\\omega$，使模型预测的输出$\\hat{y}=F(x;\\omega)$与真实的输出$y$，尽可能的接近。\n",
    "\n",
    "\n",
    "深度学习中有三种训练（学习）方式，监督学习、无监督学习、自监督学习：\n",
    "\n",
    "a. **监督学习(Supervised Learning)**：数据集中的每一个输入数据（特征）都有对应的输出（标签）；通过学习输入和输出之间的映射关系，得到模型参数。\n",
    "\n",
    "常见任务有：文本情感分类、图像分类、预测房价等\n",
    "\n",
    "b. **无监督学习 (Unsupervised Learning)**：数据集没有对应的标签，只有输入数据；模型需要自己发现数据中的潜在模式和结构。\n",
    "\n",
    "常见任务有：客户分群、主成分分析（PCA）等。\n",
    "\n",
    "\n",
    "c. **自监督学习（Self-supervised Learning）**：介于监督和无监督学习之间的一种方法。它从无标签数据中通过构造“伪标签”来进行监督学习。\n",
    "\n",
    "常见任务有：词向量学习、特征学习、预训练模型等。\n",
    "\n",
    "\n",
    "\n",
    "### 2、预训练\n",
    "\n",
    "通过从**大规模未标记数据**中学习通用特征和先验知识（**属于自监督学习**），常见预训练模型有：BRET, GPT, ResNet, VGGNet ...\n",
    "\n",
    " \n",
    "以GPT（Generative Pre-trained Transformer）模型为例：它的预训练任务是给定一个词序列，模型通过学习预测序列中下一个词\n",
    "\n",
    "\n",
    "<img src=\"pre-training.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "   \n",
    "\n",
    "### 3、微调\n",
    "\n",
    "\n",
    "通过在新任务的**小规模标注数据集**上进一步训练和调整模型的部分或全部参数，使模型能够更好地适应新任务，提高在新任务上的性能。\n",
    "\n",
    "\n",
    "<img src=\"pre-training-vs-fine-tuning.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "\n",
    "    \n",
    "预训练就像一个人读完小学、中学、大学的过程，掌握基础知识、技能和逻辑推理能力。\n",
    "    \n",
    "    \n",
    "微调就像一个人工作后，根据公司的需求深耕某块技术和业务\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7b6ab",
   "metadata": {},
   "source": [
    "## 为什么要对模型进行微调\n",
    "\n",
    "- 性价比高：因为大模型的参数量非常大，训练成本非常高，每家公司都去从头训练一个自己的大模型，这个事情的性价比非常低\n",
    "\n",
    "- 特定领域需求：在原始模型上进行Prompt Engineering的效果达不到要求，企业又有比较好的私有数据，能够通过私有数据，更好的提升大模型在特定领域的能力\n",
    "\n",
    "- 个性化服务：要在个性化的服务中使用大模型的能力，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案（Apple Intelligence）\n",
    "\n",
    "- ...\n",
    "\n",
    "\n",
    "\n",
    "**微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714db05",
   "metadata": {},
   "source": [
    "## 微调的两条技术路线\n",
    "\n",
    "### 1. 全量微调 FFT(Full Fine Tuning)\n",
    "\n",
    "(1) FFT的原理，就是用特定的数据，对大模型进行训练，将$\\omega$变成$\\omega'$，其最大的优点就是上述特定数据领域的表现会好很多\n",
    "\n",
    "(2) 存在的问题：\n",
    "\n",
    "    a. 训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的\n",
    "\n",
    "    b. 灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差\n",
    "\n",
    "\n",
    "### 2. 轻量化微调 PEFT(Parameter-Efficient Fine Tuning)\n",
    "\n",
    "\n",
    "> [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\n",
    "\n",
    "(1) 监督式微调SFT(Supervised Fine Tuning) ，这个方案主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调\n",
    "\n",
    "(2) 基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback)\n",
    "\n",
    "(3) 基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a85e7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T01:12:39.363708Z",
     "start_time": "2024-09-25T01:12:39.356731Z"
    }
   },
   "source": [
    "## 轻量化微调\n",
    "\n",
    "<img src=\"peft_process.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "注入参数的方法有（了解）：\n",
    "\n",
    "\n",
    "### 1、Prompt Tuning\n",
    "\n",
    "- 在输入序列前，额外加入一组伪 Embedding 向量\n",
    "- 只训练这组伪 Embedding，从而达到参数微调的效果\n",
    "\n",
    "<img src=\"soft-prompt.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 2、P-Tuning\n",
    "\n",
    "- 用一个生成器生成上述伪 Embedding\n",
    "- 只有生成器的参数是可训练的\n",
    "\n",
    "<img src=\"pt.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 3、Prefix-Tuning\n",
    "\n",
    "- 伪造前面的 Hidden States\n",
    "- 只训练伪造的这个 Prefix\n",
    "\n",
    "<img src=\"pt2.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "### 4、LoRA\n",
    "\n",
    "- 在 Transformer 的参数矩阵上加一个低秩矩阵（$A\\times B$）\n",
    "- 只训练 A，B\n",
    "- 理论上可以把上述方法应用于 Transformer 中的任意参数矩阵，包括 Embedding 矩阵\n",
    "- 通常应用于 Query, Value 两个参数矩阵\n",
    "\n",
    "<img src=\"lora.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 5、QLoRA\n",
    "\n",
    "什么是模型量化\n",
    "\n",
    "<img src=\"float.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "<img src=\"quant.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "更多参考: https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "\n",
    "QLoRA 引入了许多创新来在不牺牲性能的情况下节省显存：\n",
    "\n",
    "- 4位 NormalFloat（NF4），一种对于正态分布权重而言信息理论上最优的新数据类型\n",
    "- 双重量化，通过量化量化常数来减少平均内存占用\n",
    "- 分页优化器，用于管理内存峰值\n",
    "\n",
    "原文实现：单个48G的GPU显卡上微调65B的参数模型，保持16字节微调任务的性能\n",
    "\n",
    "### 6、AdaLoRA\n",
    "\n",
    "- 不预先指定可训练矩阵的秩\n",
    "- 根据参数矩阵的重要性得分，在参数矩阵之间自适应地分配参数预算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db2a26",
   "metadata": {},
   "source": [
    "## 微调实验\n",
    "\n",
    "实验代码地址：https://github.com/fangxy926/llm-learn/tree/main/9-Fine-Tuning/qwen2.5-fine-tuning\n",
    "\n",
    "### 1、实验环境准备\n",
    "\n",
    "服务器：10.104.60.47，Tesla T4单卡16G显存\n",
    "\n",
    "<img src=\"t4-server.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "\n",
    "服务器上已经配置好一个虚拟环境‘py310‘，可以使用命令`conda activate py310`激活环境后直接使用。如果是自己配置环境，需要满足以下要求：\n",
    "\n",
    "（1）Python版本：3.10及以上\n",
    "\n",
    "（2）需要安装的库：\n",
    "\n",
    "```bash\n",
    "pip install torch==2.4.1\n",
    "pip install modelscope==1.18.0\n",
    "pip install transformers==4.37.0\n",
    "pip install accelerate==0.34.2\n",
    "pip install peft==0.12.0\n",
    "pip install tensorboard==2.17.1\n",
    "\n",
    "```\n",
    "\n",
    "### 2、模型准备\n",
    "\n",
    "本次微调实验基于**qwen2.5-1.5B-Instruct**模型，模型介绍见：https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "huggingface介绍：huggingface是大模型领域的github，可以用于下载开源模型、数据等\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    \n",
    "Qwen2.5-1.5B VS Qwen2.5-1.5B-Instruct\n",
    "    \n",
    "    \n",
    "- Qwen2.5-1.5B是预训练模型, 适用于需要大量文本生成、补全、总结等广泛的自然语言处理任务，但不一定是直接的任务执行型模型。\n",
    "- Qwen2.5-1.5B-Instruct是指令微调模型， 更适用于交互式任务，如问答系统、助手型应用、对话系统等，特别是在需要理解和执行指令的场景中表现优异。\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "运行`model_download.py`将预训练模型下载到本地，下载成功后用`AutoModelForCausalLM.from_pretrained()`方法加载模型，并打印模型的结构信息\n",
    "\n",
    "```python\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = 'qwen/Qwen2.5-1.5B-Instruct'\n",
    "model_dir = './model'\n",
    "\n",
    "# 使用modelscope镜像加速下载\n",
    "model_dir = snapshot_download(model_id, cache_dir=model_dir, revision='master')\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained('./model/qwen/Qwen2___5-1___5B-Instruct/', device_map=\"auto\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "打印模型结构信息\n",
    "\n",
    "```bash\n",
    "Qwen2ForCausalLM(\n",
    "  (model): Qwen2Model(\n",
    "    (embed_tokens): Embedding(151936, 1536)\n",
    "    (layers): ModuleList(\n",
    "      (0-27): 28 x Qwen2DecoderLayer(\n",
    "        (self_attn): Qwen2SdpaAttention(\n",
    "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
    "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
    "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
    "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
    "          (rotary_emb): Qwen2RotaryEmbedding()\n",
    "        )\n",
    "        (mlp): Qwen2MLP(\n",
    "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
    "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
    "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Qwen2RMSNorm()\n",
    "        (post_attention_layernorm): Qwen2RMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): Qwen2RMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed346",
   "metadata": {},
   "source": [
    "### 3、数据集准备\n",
    "\n",
    "数据集采用 GitHub 上开源的中文医疗对话数据集：[Toyhom/Chinese-medical-dialogue-data: Chinese medical dialogue data 中文医疗对话数据集 (github.com)](https://github.com/Toyhom/Chinese-medical-dialogue-data)\n",
    "\n",
    "\n",
    "该数据集分了 6 个科目类型\n",
    "\n",
    "<img src=\"dataset_cat.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "数据格式如下，其中 ask 为病症的问题描述，answer 为病症的回答，本实验仅用到ask和answer数据。\n",
    "\n",
    "<img src=\"dataset_example.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "实验选用~~`儿科5-14000.csv`、`外科5-14000.csv`、`内科5000-33000.csv`三个~~ `样例_内科5000-6000.csv`数据集，并按8：1：1的比例将数据集划分为训练集、验证集、测试集。\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "\n",
    "    \n",
    "机器学习相关知识补充：训练集 vs 验证集 vs 测试集\n",
    "\n",
    "- 训练集（Training Set）：用来训练模型的数据集\n",
    "\n",
    "- 验证集（Validation Set）：在模型训练期间，用于评估模型表现的数据集，不用于调整模型权重\n",
    "    \n",
    "- 测试集（Test Set）：完全独立于训练集和验证集，用于最终评估模型的性能。\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "详细代码见`data_generator.py`，将数据集转为 json 格式方便后续读取\n",
    "\n",
    "处理之后可以看到三个生成的文件：`train.json`、 `val.json`、`test.json`，数据格式如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"question\": \"我有高血压这两天女婿来的时候给我拿了些党参泡水喝，您好高血压可以吃党参吗？\",\n",
    "    \"answer\": \"高血压病人可以口服党参的。党参有降血脂，降血压的作用，可以彻底消除血液中的垃圾，从而对冠心病以及心血管疾病的患者都有一定的稳定预防工作作用，因此平时口服党参能远离三高的危害。另外党参除了益气养血，降低中枢神经作用，调整消化系统功能，健脾补肺的功能。感谢您的进行咨询，期望我的解释对你有所帮助。\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23faa82",
   "metadata": {},
   "source": [
    "### 4、数据预处理\n",
    "\n",
    "\n",
    "数据拼接方式\n",
    "\n",
    "<img src=\"batch.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "详细代码见`qwen_dataset.py`，核心代码如下：\n",
    "\n",
    "```python\n",
    "def preprocess(self, question, answer):\n",
    "    # 构建对话模板，包括系统角色的描述和用户的问题\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个医疗方面的专家，可以根据患者的问题进行解答。\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    # 使用tokenizer将对话模板转换为prompt\n",
    "    prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # 对问题进行编码，生成instruction的input_ids和attention_mask\n",
    "    instruction = self.tokenizer(prompt, add_special_tokens=False, max_length=self.max_source_length)\n",
    "    # 对答案进行编码，生成response的input_ids和attention_mask\n",
    "    response = self.tokenizer(answer, add_special_tokens=False, max_length=self.max_target_length)\n",
    "    # 拼接instruction和response的input_ids，并在末尾添加padding token id\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [self.tokenizer.pad_token_id]\n",
    "    # 拼接instruction和response的attention_mask，并在末尾添加1，表示这些位置都是有效信息\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    # 生成标签序列，问题部分的标签设置为-100（表示忽略），答案部分的标签为答案的token id，末尾添加padding token id\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [self.tokenizer.pad_token_id]\n",
    "    # 如果输入序列的长度超过了最大序列长度，进行截断\n",
    "    if len(input_ids) > self.max_seq_length:\n",
    "        input_ids = input_ids[:self.max_seq_length]\n",
    "        attention_mask = attention_mask[:self.max_seq_length]\n",
    "        labels = labels[:self.max_seq_length]\n",
    "    # 返回处理后的输入序列，注意力掩码和标签序列\n",
    "    return input_ids, attention_mask, labels\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b639ce",
   "metadata": {},
   "source": [
    "### 5、微调训练\n",
    "\n",
    "主函数代码：`train.py`，运行参数脚本：`lora_train.sh`\n",
    "\n",
    "```shell\n",
    "#! /usr/bin/env bash\n",
    "\n",
    "set -ex\n",
    "\n",
    "LR=1e-4\n",
    "MAX_SEQ_LEN=384\n",
    "MAX_SOURCE_LEN=128\n",
    "MAX_TARGET_LEN=256\n",
    "\n",
    "BASE_MODEL_PATH=model/qwen/Qwen2___5-1___5B-Instruct\n",
    "\n",
    "DATESTR=`date +%Y%m%d-%H%M%S`\n",
    "RUN_NAME=qwen2.5-1.5B-Instruct-lora\n",
    "OUTPUT_DIR=output/${RUN_NAME}-${DATESTR}\n",
    "mkdir -p $OUTPUT_DIR\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file data/train.json \\\n",
    "    --validation_file data/val.json \\\n",
    "    --max_seq_length $MAX_SEQ_LEN \\\n",
    "    --max_source_length $MAX_SOURCE_LEN \\\n",
    "    --max_target_length $MAX_TARGET_LEN \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --model_name_or_path $BASE_MODEL_PATH \\\n",
    "    --output_dir $OUTPUT_DIR \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --num_train_epochs 4 \\\n",
    "    --gradient_accumulation_steps 4\\\n",
    "    --evaluation_strategy steps \\\n",
    "    --eval_steps 1000 \\\n",
    "    --logging_steps 10 \\\n",
    "    --logging_dir $OUTPUT_DIR/logs \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate $LR \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --lora_dropout 0.1 2>&1 | tee ${OUTPUT_DIR}/train.log\n",
    "\n",
    "```\n",
    "\n",
    "上述配置微调完成大约耗时6h，运行过程：\n",
    "\n",
    "<img src=\"training2.png\" style=\"margin-left: 0px\" width=\"1000px\">\n",
    "\n",
    "微调结果和日志将会保存在output目录下\n",
    "\n",
    "<img src=\"output.png\" style=\"margin-left: 0px\" width=\"1000px\">\n",
    "\n",
    "其中：\n",
    "\n",
    "- adapter_config.json：保存微调模型所需的超参数和结构信息\n",
    "\n",
    "- adapter_model.safetensors：微调后的模型权重文件\n",
    "\n",
    "- checkpoint-xxx：训练过程中的模型快照，通常会存储当前训练状态，以便恢复训练或进行评估\n",
    "\n",
    "- trian.log：训练过程中的日志\n",
    "\n",
    "- runs：执行下列命令，使用tensorboard可视化训练过程：\n",
    "\n",
    "\n",
    "```\n",
    "tensorboard --logdir=output/qwen2.5-1.5B-Instruct-lora-20241001-153846/runs/\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"train_loss.png\" style=\"margin-left: 0px\" width=\"1000px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691bd2d",
   "metadata": {},
   "source": [
    "### 6、模型测试\n",
    "\n",
    "运行`chat.py`代码，从`test.json`中随机选了3个问题\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"question\": \"最近一段时间天气忽冷忽热的，我就有些感冒了，到医院去化验了一下，大夫给开了许多抗病毒的口服液，我想进行咨询一下，抗病毒口服液可以和感冒药一同吃吗？会不会有事不良的反映？\",\n",
    "    \"answer\": \"感冒是感冒病毒导致的，临床表现足见头痛，发烫，打嗝，流鼻涕等症状，如果医生仔细检查已经开了抗病毒口服液医治，就不建议再互相配合其他的药物，防止导致药物重复导致副作用的再次发生，服药医治的同时，建议注意休息，多喝水，防止疲劳，有助于感冒全愈。\"\n",
    "}\n",
    "{\n",
    "    \"question\": \"谷丙转氨酶偏高到了141谷草转氨酶到了150您好这是什么征兆啊\",\n",
    "    \"answer\": \"谷丙转氨酶和谷草转氨酶升高，证明细胞遭到伤损或是细胞膜通透性增强，施放到血液中，谷丙转氨酶在肝脏中含量较高，心肌细胞中谷草转氨酶含量较高。仅仅只有这两项指标，不好推测，就融合临床诊断症状加以分析。爱滋病患者免疫功能扭过，容易传染肝炎病毒。病毒性肝炎的治疗方法有许多，但是由于患者症状不同所以采用的医治方法也就不一样，因此建议患者发觉症状后，及早实施诊断救治。。\"\n",
    "}{\n",
    "    \"question\": \"叔叔年纪大了，有高血压的毛病，他不爱吃晚饭，高血压不吃晚饭好吗？\",\n",
    "    \"answer\": \"高血压不吃晚饭对血压是没影响，但是对人体健康是不利的，因为不吃晚饭容易再次出现空腹，致使胃酸排泄增强致使胃肠疾病，也容易引致低血糖，也会影响人体深度睡眠。所以晚饭是一定要吃的，吃点清淡易消化饮食，别吃太饱，也别吃油腻油炸食物。饮食一定要规律，低盐低脂饮食，多吃新鲜蔬菜水果的高血压的控制有一定的好处。\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "原始模型回答结果：\n",
    "\n",
    "<img src=\"chat_base_test.png\" style=\"margin-left: 0px\" width=\"1000px\">\n",
    "\n",
    "\n",
    "微调后模型回答结果：\n",
    "\n",
    "\n",
    "<img src=\"chat_test.png\" style=\"margin-left: 0px\" width=\"1000px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dda60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
