{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbee847",
   "metadata": {},
   "source": [
    "## 模型的本质\n",
    "\n",
    "通俗（不严谨）的讲：**模型**是包含输入、输出的函数表达式\n",
    "\n",
    "$y=F(x;\\omega)$\n",
    "\n",
    "- 输入$x$：可以是一个词、一个句子、一篇文章或图片、语音、视频 ... 大模型中输入都被表示成一个矩阵（专业名词叫**张量**，tensor）\n",
    "  \n",
    "- 输出$y$：可以是「是否」（{0,1}）、标签（{0,1,2,3...}）、一个数值（回归问题）、下一个词的概率 ...\n",
    "\n",
    "- 表达式$F$ 就是神经网络结构（这里特指深度学习）\n",
    "\n",
    "- 它有一组**参数** $\\omega$，这就是我们要训练的部分\n",
    "\n",
    "\n",
    "\n",
    "通俗的讲：训练模型，就是确定这组参数的取值\n",
    "\n",
    "- 用数学（数值分析）方法找到使模型在训练集上表现足够好的一个值</li>\n",
    "- 表现足够好，就是说，对每个数据样本$(x,y)$，使 $F(x;\\omega)$ 的值尽可能接近 $y$</li>\n",
    "\n",
    "所谓的“大模型”，“大”在训练模型的参数非常多，多达7B（70亿）、72B（720亿）...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f35b6d",
   "metadata": {},
   "source": [
    "## 模型训练 vs 预训练 vs 微调\n",
    "\n",
    "\n",
    "### 1、训练\n",
    "\n",
    "利用给定的数据集，**从无到有**地训练出一个模型；我们希望找到一组参数$\\omega$，使模型预测的输出$\\hat{y}=F(x;\\omega)$与真实的输出$y$，尽可能的接近。\n",
    "\n",
    "\n",
    "深度学习中有三种训练（学习）方式，监督学习、无监督学习、自监督学习：\n",
    "\n",
    "a. **监督学习(Supervised Learning)**：数据集中的每一个输入数据（特征）都有对应的输出（标签）；通过学习输入和输出之间的映射关系，得到模型参数。\n",
    "\n",
    "常见任务有：文本情感分类、图像分类、预测房价等\n",
    "\n",
    "b. **无监督学习 (Unsupervised Learning)**：数据集没有对应的标签，只有输入数据；模型需要自己发现数据中的潜在模式和结构。\n",
    "\n",
    "常见任务有：客户分群、主成分分析（PCA）等。\n",
    "\n",
    "\n",
    "c. **自监督学习（Self-supervised Learning）**：介于监督和无监督学习之间的一种方法。它从无标签数据中通过构造“伪标签”来进行监督学习。\n",
    "\n",
    "常见任务有：词向量学习、特征学习、预训练模型等。\n",
    "\n",
    "\n",
    "\n",
    "### 2、预训练\n",
    "\n",
    "通过从**大规模未标记数据**中学习通用特征和先验知识（**属于自监督学习**），常见预训练模型有：BRET, GPT, ResNet, VGGNet ...\n",
    "\n",
    " \n",
    "以GPT（Generative Pre-trained Transformer）模型为例：它的预训练任务是给定一个词序列，模型通过学习预测序列中下一个词\n",
    "\n",
    "\n",
    "<img src=\"pre-training.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "   \n",
    "\n",
    "### 3、微调\n",
    "\n",
    "\n",
    "通过在新任务的**小规模标注数据集**上进一步训练和调整模型的部分或全部参数，使模型能够更好地适应新任务，提高在新任务上的性能。\n",
    "\n",
    "\n",
    "<img src=\"pre-training-vs-fine-tuning.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "\n",
    "    \n",
    "预训练就像一个人读完小学、中学、大学的过程，掌握基础知识、技能和逻辑推理能力。\n",
    "    \n",
    "    \n",
    "微调就像一个人工作后，根据公司的需求深耕某块技术和业务\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7b6ab",
   "metadata": {},
   "source": [
    "## 为什么要对模型进行微调\n",
    "\n",
    "- 性价比高：因为大模型的参数量非常大，训练成本非常高，每家公司都去从头训练一个自己的大模型，这个事情的性价比非常低\n",
    "\n",
    "- 特定领域需求：在原始模型上进行Prompt Engineering的效果达不到要求，企业又有比较好的私有数据，能够通过私有数据，更好的提升大模型在特定领域的能力\n",
    "\n",
    "- 个性化服务：要在个性化的服务中使用大模型的能力，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案（Apple Intelligence）\n",
    "\n",
    "- ...\n",
    "\n",
    "\n",
    "\n",
    "**微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714db05",
   "metadata": {},
   "source": [
    "## 微调的两条技术路线\n",
    "\n",
    "### 1. 全量微调 FFT(Full Fine Tuning)\n",
    "\n",
    "(1) FFT的原理，就是用特定的数据，对大模型进行训练，将$\\omega$变成$\\omega'$，其最大的优点就是上述特定数据领域的表现会好很多\n",
    "\n",
    "(2) 存在的问题：\n",
    "\n",
    "    a. 训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的\n",
    "\n",
    "    b. 灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差\n",
    "\n",
    "\n",
    "### 2. 轻量化微调 PEFT(Parameter-Efficient Fine Tuning)\n",
    "\n",
    "\n",
    "> [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)\n",
    "\n",
    "(1) 监督式微调SFT(Supervised Fine Tuning) ，这个方案主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调\n",
    "\n",
    "(2) 基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback)\n",
    "\n",
    "(3) 基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a85e7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T01:12:39.363708Z",
     "start_time": "2024-09-25T01:12:39.356731Z"
    }
   },
   "source": [
    "## 轻量化微调\n",
    "\n",
    "<img src=\"peft_process.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "\n",
    "\n",
    "注入参数的方法有\n",
    "\n",
    "\n",
    "### 1、Prompt Tuning\n",
    "\n",
    "- 在输入序列前，额外加入一组伪 Embedding 向量\n",
    "- 只训练这组伪 Embedding，从而达到参数微调的效果\n",
    "\n",
    "<img src=\"soft-prompt.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 2、P-Tuning\n",
    "\n",
    "- 用一个生成器生成上述伪 Embedding\n",
    "- 只有生成器的参数是可训练的\n",
    "\n",
    "<img src=\"pt.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 3、Prefix-Tuning\n",
    "\n",
    "- 伪造前面的 Hidden States\n",
    "- 只训练伪造的这个 Prefix\n",
    "\n",
    "<img src=\"pt2.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "### 4、LoRA\n",
    "\n",
    "- 在 Transformer 的参数矩阵上加一个低秩矩阵（$A\\times B$）\n",
    "- 只训练 A，B\n",
    "- 理论上可以把上述方法应用于 Transformer 中的任意参数矩阵，包括 Embedding 矩阵\n",
    "- 通常应用于 Query, Value 两个参数矩阵\n",
    "\n",
    "<img src=\"lora.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 5、QLoRA\n",
    "\n",
    "什么是模型量化\n",
    "\n",
    "<img src=\"float.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "<img src=\"quant.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "更多参考: https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "\n",
    "QLoRA 引入了许多创新来在不牺牲性能的情况下节省显存：\n",
    "\n",
    "- 4位 NormalFloat（NF4），一种对于正态分布权重而言信息理论上最优的新数据类型\n",
    "- 双重量化，通过量化量化常数来减少平均内存占用\n",
    "- 分页优化器，用于管理内存峰值\n",
    "\n",
    "原文实现：单个48G的GPU显卡上微调65B的参数模型，保持16字节微调任务的性能\n",
    "\n",
    "### 6、AdaLoRA\n",
    "\n",
    "- 不预先指定可训练矩阵的秩\n",
    "- 根据参数矩阵的重要性得分，在参数矩阵之间自适应地分配参数预算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db2a26",
   "metadata": {},
   "source": [
    "## 微调实验\n",
    "\n",
    "实验代码地址：\n",
    "\n",
    "### 1、实验环境准备\n",
    "\n",
    "```bash\n",
    "pip install torch==2.4.1\n",
    "pip install modelscope==1.18.0\n",
    "pip install transformers==4.37.0\n",
    "pip install accelerate==0.34.2\n",
    "pip install peft==0.12.0\n",
    "pip install tensorboard==2.17.1\n",
    "\n",
    "```\n",
    "\n",
    "### 2、模型准备\n",
    "\n",
    "本次实验尝试微调qwen2.5-0.5B模型，模型介绍见：https://huggingface.co/Qwen/Qwen2.5-0.5B\n",
    "\n",
    "运行`model_download.py`将预训练模型下载到本地，下载成功后用`AutoModelForCausalLM.from_pretrained()`方法加载模型，并打印模型的结构信息\n",
    "\n",
    "```python\n",
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = 'qwen/Qwen2.5-0.5B'\n",
    "model_dir = './model'\n",
    "\n",
    "# 使用modelscope镜像加速下载\n",
    "model_dir = snapshot_download(model_id, cache_dir=model_dir, revision='master')\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained('./model/qwen/Qwen2___5-0___5B/', device_map=\"auto\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "模型结构\n",
    "\n",
    "```bash\n",
    "Qwen2ForCausalLM(\n",
    "  (model): Qwen2Model(\n",
    "    (embed_tokens): Embedding(151936, 896)\n",
    "    (layers): ModuleList(\n",
    "      (0-23): 24 x Qwen2DecoderLayer(\n",
    "        (self_attn): Qwen2SdpaAttention(\n",
    "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
    "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
    "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
    "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
    "          (rotary_emb): Qwen2RotaryEmbedding()\n",
    "        )\n",
    "        (mlp): Qwen2MLP(\n",
    "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
    "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
    "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Qwen2RMSNorm()\n",
    "        (post_attention_layernorm): Qwen2RMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): Qwen2RMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed346",
   "metadata": {},
   "source": [
    "### 3、数据集准备\n",
    "\n",
    "数据集采用 GitHub 上的 Chinese-medical-dialogue-data 中文医疗对话数据集：[Toyhom/Chinese-medical-dialogue-data: Chinese medical dialogue data 中文医疗对话数据集 (github.com)](https://github.com/Toyhom/Chinese-medical-dialogue-data)\n",
    "\n",
    "\n",
    "数据分了 6 个科目类型\n",
    "\n",
    "<img src=\"dataset_cat.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "数据格式如下，其中 ask 为病症的问题描述，answer 为病症的回答，本实验仅用到ask和answer数据。\n",
    "\n",
    "<img src=\"dataset_example.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "\n",
    "实验用样例数据`样例_内科5000-6000.csv`，取 5000 条数据进行训练、2000 条数据进行验证。\n",
    "\n",
    "\n",
    "首先将数据集转为 json 格式方便后续读取，详细代码见`data_generator.py`\n",
    "\n",
    "处理之后可以看到两个生成的文件：train.json, val.json，数据格式如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"question\": \"我有高血压这两天女婿来的时候给我拿了些党参泡水喝，您好高血压可以吃党参吗？\",\n",
    "    \"answer\": \"高血压病人可以口服党参的。党参有降血脂，降血压的作用，可以彻底消除血液中的垃圾，从而对冠心病以及心血管疾病的患者都有一定的稳定预防工作作用，因此平时口服党参能远离三高的危害。另外党参除了益气养血，降低中枢神经作用，调整消化系统功能，健脾补肺的功能。感谢您的进行咨询，期望我的解释对你有所帮助。\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd389c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
