{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写在前面\n",
    "\n",
    "- LangChain 也是一套面向大模型的开发框架（SDK）\n",
    "- LangChain 并不完美，还在不断迭代中\n",
    "- 学习 Langchain 更重要的是借鉴其思想，具体的接口和模块可能很快就会改变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain vs. Semantic Kernel\n",
    "\n",
    "[![Star History Chart](https://api.star-history.com/svg?repos=langchain-ai/langchain,microsoft/semantic-kernel&type=Date)](https://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&Date)\n",
    "\n",
    "\n",
    "LangChain 完胜？我们接下来仔细看看。\n",
    "\n",
    "\n",
    "| 功能/工具           | LangChain                       | Semantic Kernel                  |\n",
    "|-------------------|:---------------------------------:|:----------------------------------:|\n",
    "| 版本号        |  0.1.0  | python-0.4.4.dev  |\n",
    "| 适配的 LLM        | 多   | 少 + 外部生态   |\n",
    "| Prompt 工具        | 支持    | 支持     |\n",
    "| Prompt 函数嵌套    | 需要通过 LCEL | 支持        |\n",
    "| Prompt 模板嵌套    | 不支持  | 不支持       |\n",
    "| 输出解析工具       | 支持  | 不支持  |\n",
    "| 上下文管理工具           | 支持 | C#版支持，Python版尚未支持  |\n",
    "| 内置工具           | 多，但良莠不齐  | 少 + 外部生态  |\n",
    "| 三方向量数据库适配           | 多 | 少 + 外部生态  |\n",
    "| 服务部署 | LangServe | 与 Azure 衔接更丝滑\n",
    "| 管理工具 | LangSmith/LangFuse | Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangChain 的核心组件\n",
    "\n",
    "<img src=\"langchain_stack.png\" style=\"margin-left: 0px\" width=1000px>\n",
    "\n",
    "1. 模型 I/O 封装\n",
    "   - LLMs：大语言模型\n",
    "   - Chat Models：一般基于 LLMs，但按对话结构重新封装\n",
    "   - PromptTemple：提示词模板\n",
    "   - OutputParser：解析输出\n",
    "2. 数据连接封装\n",
    "   - Document Loaders：各种格式文件的加载器\n",
    "   - Document Transformers：对文档的常用操作，如：split, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Models：文本向量化表示，用于检索等操作（啥意思？别急，后面详细讲）\n",
    "   - Verctorstores: （面向检索的）向量的存储\n",
    "   - Retrievers: 向量的检索\n",
    "3. 记忆封装\n",
    "   - Memory：这里不是物理内存，从文本的角度，可以理解为“上文”、“历史记录”或者说“记忆力”的管理\n",
    "4. 架构封装\n",
    "\n",
    "   - Chain：实现一个功能或者一系列顺序功能组合\n",
    "\n",
    "   - Agent：根据用户输入，自动规划执行步骤，自动选择每步需要的工具，最终完成用户指定的功能\n",
    "     - Tools：调用外部功能的函数，例如：调 google 搜索、文件 I/O、Linux Shell 等等\n",
    "     - Toolkits：操作某软件的一组工具集，例如：操作 DB、操作 Gmail 等等\n",
    "\n",
    "   - Chain及Agent架构设计（下图为ReAct类型智能体，后文会介绍）\n",
    "   \n",
    "   <img src=\"langchain.png\" style=\"margin-left: 0px\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、模型 I/O 封装\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/model_io/\n",
    "\n",
    "\n",
    "把不同的模型，统一封装成一个接口，方便更换模型而不用重构代码。\n",
    "\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "在Model I/O这一流程中，LangChain抽象的组件主要有三个：\n",
    "\n",
    "- Language models\n",
    "- Prompts\n",
    "- Output parsers\n",
    "\n",
    "\n",
    "\n",
    "安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#安装最新版本\n",
    "!pip install langchain==0.2.13\n",
    "!pip install langchain-openai\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 模型封装：LLM vs. ChatModel\n",
    "\n",
    "LangChain中提供了三类模型的封装，LLM、ChatModel和Embedding，下面聊一下LLM和ChatModel的区别：\n",
    "\n",
    "（1）LLM：输入和输出都是**纯文本**的模型（text in， text out）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n我是一个人工智能助手，被设计来回答问题、提供帮助和进行对话。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "text = \"你是谁？\"\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）ChatModel：是LLM的变体，对聊天场景进行了抽象。输入不是纯文本，而是chat message列表，输出也是chat message。\n",
    "\n",
    "    chat message = text + 消息类型(System, Human, AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你是张三。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 26, 'total_tokens': 31}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-25c56638-f0f4-4a8c-af2e-62149d92712f-0', usage_metadata={'input_tokens': 26, 'output_tokens': 5, 'total_tokens': 31})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "chat_model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chat message目前一共有5种类型，每种message至少包含角色（role）和内容（content）两个参数：\n",
    "\n",
    "    - HumanMessage：等价于OpenAI接口中的user role\n",
    "\n",
    "    - AIMessage：等价于OpenAI接口中的assistant role\n",
    "    \n",
    "    - SystemMessage：等价于OpenAI接口中的system role\n",
    "\n",
    "    - FunctionMessage：function call的结果。除了role和content之外，它还有一个name参数，表示函数的名称。\n",
    "    \n",
    "    - ToolMessage：tool call的结果。除了role和content之外，它还有一个tool_call_id参数。\n",
    "\n",
    "    详细文档：https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您是张三，一位学员。您可以告诉我您有什么问题或者需要帮助的地方。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 51, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bcb315c1-bf88-473f-b855-182c3bfe5b15-0', usage_metadata={'input_tokens': 51, 'output_tokens': 33, 'total_tokens': 84})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是AGIClass的课程助理。\"), \n",
    "    HumanMessage(content=\"我是学员，我叫张三\"),\n",
    "    AIMessage(content=\"欢迎！\"),\n",
    "    HumanMessage(content=\"我是谁？\")\n",
    "]\n",
    "\n",
    "chat_model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "（3）**为什么要区分LLM和ChatModel？**\n",
    "\n",
    "- LLM主要处理一次性问题，适用于回答问题、生成文本等场景\n",
    "- Chat Model 则专注于多轮对话场景，不仅接受用户的输入，还考虑对话的上下文\n",
    "\n",
    "\n",
    "（4）通过模型封装，可以实现不同模型的统一接口调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 从langchain_community导入百度千帆大模型\n",
    "from langchain_community.chat_models import ErnieBotChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "ernie = ErnieBotChat()\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"你是谁\") \n",
    "]\n",
    "\n",
    "ernie.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt模板封装\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
    "\n",
    "#### 1.2.1 PromptTemplate\n",
    "- PromptTemplate是纯字符串形式\n",
    "- 使用类似于str.format()语法进行参数补全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject']\n",
      "给我讲个关于小明的笑话\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# 参数用{}包裹\n",
    "template = PromptTemplate.from_template(\"给我讲个关于{subject}的笑话\")\n",
    "print(template.input_variables)\n",
    "print(template.format(subject='小明'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 ChatPromptTemplate\n",
    "- ChatPromptTemplate是对chat message的封装，携带角色信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是AGI课堂的客服助手。你的名字叫瓜瓜'), HumanMessage(content='你是谁')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 等价于 {\"role\": \"system\", \"content\": \"你是{product}的客服助手。你的名字叫{name}\"},\n",
    "        SystemMessagePromptTemplate.from_template(\"你是{product}的客服助手。你的名字叫{name}\"),\n",
    "        # 等价于 {\"role\": \"user\", \"content\": \"{query}\"},\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "        product=\"AGI课堂\",\n",
    "        name=\"瓜瓜\",\n",
    "        query=\"你是谁\"\n",
    "    )\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 从文件加载Prompt模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Yaml格式\n",
    "\n",
    "``` yml\n",
    " _type: prompt\n",
    "input_variables:\n",
    "    [\"adjective\", \"content\"]\n",
    "template: \n",
    "    Tell me a {adjective} joke about {content}.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. JSON格式\n",
    "\n",
    "```JSON\n",
    "\n",
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template\": \"Tell me a {adjective} joke about {content}.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Template单独存放\n",
    "\n",
    "- 注意点：使用`template_path`\n",
    "- 从代码文件的相对路径找模板文件\n",
    "\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template_path\": \"simple_template.txt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about fox.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"simple_prompt.yaml\")\n",
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "\n",
    "print(prompt.format(adjective=\"funny\", content=\"fox\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 输出封装 OutputParser\n",
    "\n",
    "自动把 LLM 输出的字符串转换为指定格式\n",
    "\n",
    "LangChain 内置的 OutputParser 包括:\n",
    "\n",
    "- PydanticParser\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- XMLParser\n",
    "\n",
    "等等， 官方文档：https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/\n",
    "\n",
    "### 1.3.1 Pydantic (JSON) Parser\n",
    "\n",
    "- Pydantic 是一个用于数据验证和数据解析的 Python 库。\n",
    "- PydanticParser可以自动根据Pydantic类的定义，**生成JSON格式的输出**。\n",
    "- Pydantic v1版本即将弃用，建议使用v2版本：https://python.langchain.com/v0.2/docs/how_to/pydantic_compatibility/\n",
    "- 使用PydanticParser的好处，不需要在prompt中描述一大串json格式的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "# 定义你的输出对象\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Format Instruction=====\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"description\": \"Year\", \"title\": \"Year\", \"type\": \"integer\"}, \"month\": {\"description\": \"Month\", \"title\": \"Month\", \"type\": \"integer\"}, \"day\": {\"description\": \"Day\", \"title\": \"Day\", \"type\": \"integer\"}, \"era\": {\"description\": \"BC or AD\", \"title\": \"Era\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "text='提取用户输入中的日期。\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"year\": {\"description\": \"Year\", \"title\": \"Year\", \"type\": \"integer\"}, \"month\": {\"description\": \"Month\", \"title\": \"Month\", \"type\": \"integer\"}, \"day\": {\"description\": \"Day\", \"title\": \"Day\", \"type\": \"integer\"}, \"era\": {\"description\": \"BC or AD\", \"title\": \"Era\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\\n```\\n用户输入:\\n2023年四月6日天气晴...'\n",
      "====Output=====\n",
      "content='{\\n  \"year\": 2023,\\n  \"month\": 4,\\n  \"day\": 6,\\n  \"era\": \"AD\"\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 256, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-cc701941-4a42-4779-8157-e934b691ec8a-0' usage_metadata={'input_tokens': 256, 'output_tokens': 31, 'total_tokens': 287}\n",
      "====Parsed=====\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# 根据Pydantic对象的定义，构造一个OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "template = \"\"\"提取用户输入中的日期。\n",
    "{format_instructions}\n",
    "用户输入:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    # 预先赋值：直接从OutputParser中获取输出描述，并对模板的变量预先赋值\n",
    "    # https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()} \n",
    ")\n",
    "\n",
    "print(\"====Format Instruction=====\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "\n",
    "query = \"2023年四月6日天气晴...\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(model_input)\n",
    "\n",
    "output = model.invoke(model_input)\n",
    "print(\"====Output=====\")\n",
    "print(output)\n",
    "print(\"====Parsed=====\")\n",
    "date = parser.parse(output.content)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Auto-Fixing Parser\n",
    "\n",
    "利用LLM自动根据解析异常修复并重新解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===格式错误的Output===\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===出现异常===\n",
      "Invalid json output: {\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===重新解析结果===\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "#我们把之前output的格式改错\n",
    "output = output.content.replace(\"4\",\"四\")\n",
    "print(\"===格式错误的Output===\")\n",
    "print(output)\n",
    "try:\n",
    "    date = parser.parse(output)\n",
    "except Exception as e:\n",
    "    print(\"===出现异常===\")\n",
    "    print(e)\n",
    "    \n",
    "#用OutputFixingParser自动修复并解析\n",
    "date = new_parser.parse(output)\n",
    "print(\"===重新解析结果===\")\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "猜一下OutputFixingParser是怎么做到的？ 再调一遍大模型告诉他哪里不对，让它修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4、小结\n",
    "\n",
    "1. LangChain 统一封装了各种模型的调用接口\n",
    "2. LangChain 提供了提示词模板类，可以自定义带变量的模板\n",
    "3. LangChain 提供了一些输出解析器，用于将大模型的输出解析成结构化对象；额外带有自动修复功能。\n",
    "4. 上述模型属于 LangChain 中较为优秀的部分；美中不足的是 OutputParser 自身的 Prompt 维护在代码中，耦合度较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据连接封装\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "### 2.1 文档加载器：Document Loaders\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 加载PDF需要的\n",
    "!pip install pypdf\n",
    "# 加载docx需要的\n",
    "!pip install python-docx\n",
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"体检中心问答.docx\")\n",
    "doc = loader.load()\n",
    "content = doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'体检预约有电话预约，现场预约及网上预约三种方式。请问您需要哪种方式？\\n\\n电话预约：\\n\\n请您在工作时间08：00-17：00拨打预约热线，南白象院区：555578888 /55578800，公园路院区88069196。我们竭诚为您服务。\\n\\n现场预约：\\n\\n因体检量会有饱和情况，建议您提前来院预约。现场预约办理：请至预约中心。\\n\\n网上预约:\\n\\n请关注温医一院医疗保健中心微信公众号-体检服务-体检预约 进行预约\\n\\n或关注“温医一院”公众号--就医--健康服务--体检预约 进行预约。\\n\\n体检中心地址：\\n\\n公园路院区：温州市鹿城区府学巷96号\\n\\n南白象院区：温州市瓯海区南白象街道上蔡村温医大附一院5号楼\\n\\n\\n\\n体检方式有哪几种？\\n\\n体检方式分普通体检及入住体检两种。普通体检半天完成，如果有胃肠镜、磁共振等特殊项目，则需另约时间。入住体检有安排客房休息，专人带您做体检，胃肠镜第二天上午完成，中午左右就可以完成体检。\\n\\n\\n\\n入住体检跟普通体检有什么区别？\\n\\n入住体检套餐内容比普通体检套餐内容更加全面深度，也包含了检前咨询、24小时内完成包括胃肠镜、睡眠监测等所有套餐内项目，还有检中导医带诊、体检入住、'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 文档处理器 TextSplitter\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体检预约有电话预约，现场预约及网上预约三种方式。请问您需要哪种方式？\n",
      "\n",
      "电话预约：\n",
      "\n",
      "请您在工作时间08：00-17：00拨打预约热线，南白象院区：555578888 /55578800，公园路院区88069196。我们竭诚为您服务。\n",
      "\n",
      "现场预约：\n",
      "\n",
      "因体检量会有饱和情况，建议您提前来院预约。现场预约办理：请至预约中心。\n",
      "\n",
      "网上预约:\n",
      "\n",
      "请关注温医一院医疗保健中心微信公众号-体检服务-体检预约 进行预约\n",
      "\n",
      "或关注“温医一院”公众号--就医--健康服务--体检预约 进行预约。\n",
      "\n",
      "体检中心地址：\n",
      "\n",
      "公园路院区：温州市鹿城区府学巷96号\n",
      "\n",
      "南白象院区：温州市瓯海区南白象街道上蔡村温医大附一院5号楼\n",
      "-------\n",
      "体检方式有哪几种？\n",
      "\n",
      "体检方式分普通体检及入住体检两种。普通体检半天完成，如果有胃肠镜、磁共振等特殊项目，则需另约时间。入住体检有安排客房休息，专人带您做体检，胃肠镜第二天上午完成，中午左右就可以完成体检。\n",
      "-------\n",
      "入住体检跟普通体检有什么区别？\n",
      "\n",
      "入住体检套餐内容比普通体检套餐内容更加全面深度，也包含了检前咨询、24小时内完成包括胃肠镜、睡眠监测等所有套餐内项目，还有检中导医带诊、体检入住、健康宣教、检后专家咨询、检后随访等服务。\n",
      "-------\n",
      "体检套餐内容及价格多少？\n",
      "\n",
      "体检套餐分基础套餐，深度套餐，尊享套餐（此套餐属入住体检）。具体内容及价格请关注温医一院医疗保健中心微信公众号-体检服务-体检预约-选择体检方式后查看体检套餐价格及具体检查项目。\n",
      "-------\n",
      "体检套餐内的检查项目可以增加或者删除吗？\n",
      "\n",
      "体检套餐采用最优化组合方式，建议不要删减。如需删减或增加项目请至预约中心办理。\n",
      "-------\n",
      "体检套餐全面了？\n",
      "\n",
      "体检套餐的内容根据您的整体情况定制的，根据这个体检套餐的内容先进行初步筛查，如果有后续的问题，可以进一步深入检查。\n",
      "-------\n",
      "电话预约时可以直接选体检套餐吗？\n",
      "\n",
      "抱歉，目前我们电话里暂不支持选择体检套餐。电话预约仅预约体检时间，体检当天请至预约中心选择套餐后体检。或关注公众号“温医一院医疗保健中心”或“温医一院” 进行线上选套餐，填写问卷后系统推荐套餐。详情我们会以短信链接的方式发送，敬请关注。\n",
      "-------\n",
      "体检为什么比门诊的检查要贵？\n",
      "\n",
      "体检提供随到随检，当天完成检查项目，更加方便快捷，另有专业的医护团队提供个\n",
      "\n",
      "性化的指导咨询等全程健康管理服务。\n",
      "-------\n",
      "体检缴费方式有哪些？\n",
      "\n",
      "体检套餐制定后的缴费可使用微信、支付、银行储蓄卡刷卡支付。不建议使用现金支付。\n",
      "\n",
      "目前医院暂不支持支付宝/微信绑定的信用卡超过1000元的支付。\n",
      "-------\n",
      "体检医保能不能报销？\n",
      "\n",
      "体检不属医保范围，不能报销，需自费。\n",
      "-------\n",
      "体检能开发票吗？\n",
      "\n",
      "个人发票：5号楼一楼自助机处有自主发票打印机，有需要您可以携带身份证/就诊卡/电子卡自行前往打印。\n",
      "\n",
      "单位发票：请体检前至预约中心详细咨询。\n",
      "-------\n",
      "入住体检能报销吗？能不能开住院发票？\n",
      "\n",
      "您好，入住体检属体检的一种方式，体检不属医保范围，不能报销需自费，开不了住院发票\n",
      "-------\n",
      "体检需要带哪些证件？\n",
      "\n",
      "来院体检携带身份证（用于身份核对，财务建档，绑定身份信息及体检数据）医保卡（非必须，用于将体检检查结果关联在医保卡上，避免重复办卡）。华侨/外籍友人携带护照\n",
      "\n",
      "体检中心几点上班开始体检\n",
      "\n",
      "中心实行分时段签到。07：30-10：30按预约时段签到，如提前到达，请耐心等待。\n",
      "-------\n",
      "有提供停车位吗？\n",
      "\n",
      "本中心可提供免费停车服务，但车位有限，建议绿色出行\n",
      "-------\n",
      "体检前的注意事项有哪些？\n",
      "\n",
      "1.体检当天请按预约时间签到。如提前到达，请耐心等待。\n",
      "\n",
      "2.避免剧烈运动，保证睡眠充足。\n",
      "\n",
      "3.饮食管理：体检前3天内，尽量避免高糖、高脂肪、高蛋白的食物，避免饮酒、饮用咖啡或浓茶。体检前至少禁食8小时。\n",
      "\n",
      "4.药物管理：体检当日，不要贸然停用高血压、冠心病药物，可少量饮水服用；禁食状态暂停糖尿病用药，完成检查或进餐后及时服用。请随身携带正在服用的药物，及时告知选套餐医师病情和服药情况，以便医师进行相关体检检查项目的选择，并作出药物的服用指导。\n",
      "\n",
      "5.穿着准备：请穿舒适、易穿脱衣物，勿穿带有金属纽扣的衣服、内衣等。\n",
      "\n",
      "6.证件准备：请携带本人有效身份证或护照、社保卡。\n",
      "\n",
      "7.女性生理期：尽量选择生理期结束3-5天及以后。\n",
      "\n",
      "8.怀孕、计划怀孕者（包括男性），必须提前告知医护人员，X线、CT等有放射性的检查需禁检或慎检。\n",
      "\n",
      "9.胃肠镜检查者，需在专科医生指导下暂停抗血小板药物和抗凝药物1周，如抗血小板药物：\n",
      "\n",
      "阿司匹林，P2Y12受体拮抗剂（氯吡格雷、普拉格雷、替格瑞洛）；抗凝药物：华法林、利伐沙班、达比加群、阿帕沙班、依多沙班等。\n",
      "\n",
      "10.磁共振MRI检查者，如身上有不可拆除的金属植入物，如骨骼固定装置、心脏起搏器、血管支架、植入式人工耳蜗、金属假牙等，必须提前告知医护人员，谨慎选择MRI检查。\n",
      "\n",
      "11.幽门螺旋杆菌（C13呼气）检查者，检查前禁饮禁烟2小时，禁抗生素、枸橼酸铋钾、质子泵抑制剂1个月。\n",
      "-------\n",
      "体检过程中有导医带吗？\n",
      "\n",
      "普通体检方式我们工作人员都会在平面的各个地方提供帮助，如果您在体检过程中有需要帮助请随时联系我们各平面的导医，我们很乐意为您提供帮助。\n",
      "\n",
      "如您需要导医全程陪同体检，建议您选择入住体检方式。\n",
      "-------\n",
      "体检过程中各个检查项目怎么去排队？\n",
      "\n",
      "中心实行智能导诊系统，系统会根据排队人数的多少，自动分配到排队较少的项目，可以加快您的体检进程。\n",
      "-------\n",
      "入住体检是否能安排套房？\n",
      "\n",
      "我们会尽量为您安排，但套房有限，不予预定，如未能满足您的需求，请见谅！\n",
      "-------\n",
      "哪些检查需要空腹？\n",
      "\n",
      "抽血、胃幽门螺旋杆菌（碳13）、腹部B超、腹部CT、胃肠镜、喉镜等检查项目需空腹进行。具体检查当天请随时与我们工作人员联系。\n",
      "-------\n",
      "体检当天可以吃药吗？\n",
      "\n",
      "药物管理：体检当日，不要贸然停用高血压、冠心病药物，可少量饮水服用；禁食状态暂停糖尿病用药，完成检查或进餐后及时服用。请随身携带正在服用的药物，及时告知选套餐医师病情和服药情况，以便医师进行相关体检检查项目的选择，并作出药物的服用指导。\n",
      "-------\n",
      "空腹的项目能不能先做？\n",
      "\n",
      "体检过程中，空腹项目会优先安排。但如空腹的项目人数较多，需要排队较长时间，我们由智能导检分配到排队较少的项目，可以加快您体检的进程\n",
      "-------\n",
      "备用怀孕不能做哪些项目？\n",
      "\n",
      "如备孕或怀孕，请主动告知工作人员，X线、CT等有放射性的检查慎检。\n",
      "-------\n",
      "咨询体检异常检查结果/咨询医学专业知识\n",
      "\n",
      "抱歉，您的问题属于医学专业知识，需要专科医生解答，请拨打人工客服电话55578888/55578800\n",
      "-------\n",
      "有电话体检报告解读服务吗？\n",
      "\n",
      "抱歉，目前中心暂未提供电话解读服务。一牵涉隐私问题，体检报告要求本人及家\n",
      "\n",
      "属带身份证，现场领取，现场解读。二体检过程中发现的危急问题，医生已电话或短信告知。\n",
      "\n",
      "三您在手机微信公众号上均可以查询报告。建议您安排时间到本中心现场解读，谢谢您的理解\n",
      "-------\n",
      "有线上体检报告解读服务吗？\n",
      "\n",
      "抱歉，目前中心暂未提供电话解读服务。建议您安排时间到本中心现场解读，谢谢您的理解\n",
      "-------\n",
      "投诉流程问题或工作人员态度问题\n",
      "\n",
      "非常抱歉，让您在体检过程中有不愉快的体验。请拨打人工客服电话55578888/55578800。人工客服会详细记录您反映的情况，并进行上报处理。给您体检过程中带来的不愉快，再次表示抱歉，祝您生活愉快\n",
      "-------\n",
      "无痛胃肠镜家属不陪同可以吗？\n",
      "\n",
      "无痛胃肠镜检查是有麻醉，麻醉有风险，需要您家属或朋友的陪同。如果因特殊情况，无陪护人员，请拨打人工客服电话55578888，55578800告知工作人员。或改为普通胃肠镜检查。\n",
      "-------\n",
      "胃肠镜检查如果发现息肉会马上切除吗？\n",
      "\n",
      "胃肠镜检查中如发现息肉是否马上切除，这取决于息肉的类型、发小、数量等，检查的医生会具体问题具体分析，为您选择最佳的方案。\n",
      "-------\n",
      "胃镜一般是多长时间做一次？\n",
      "\n",
      "胃镜检查的频率要根据不同病情来确定，如果胃镜结果显示C3以上的慢性萎缩性胃炎或病理结果显示中重度以上萎缩和肠化，建议每1年检查一次。根据《胃癌诊疗规范(2018版)》，符合以下任意一条者，不管有无症状都要进行胃镜检查：\n",
      "\n",
      "1.年龄大于40岁，男女不限；\n",
      "\n",
      "2.胃癌高发地区人群;\n",
      "\n",
      "3.幽门螺杆菌感染者;\n",
      "\n",
      "4.患慢性萎缩性胃炎、胃溃疡、胃息肉等胃部疾病患者;\n",
      "\n",
      "5.家族史：胃癌患者一级亲属;\n",
      "\n",
      "6.存在胃癌其他高危因素(喜食高盐、烧烤、腌制食品、重度饮酒等)。\n",
      "-------\n",
      "肠镜一般多长时间做一次？\n",
      "\n",
      "一般建议40岁以上人群开始做肠镜检查，以筛查结肠癌。如无特殊疾病既往无息肉或无家族史，可以3-5年检查一次，可以早期发现一些病变。如既往有肠癌家族史、既往有息肉病的病人，以及肠道疾病高发人群或者一些特殊病史的病人，可以适当缩短检查间隔时间，具体筛查的时间由医生根据检查结果而定。\n",
      "-------\n",
      "胃肠镜检查普通有痛可以改无痛吗？\n",
      "\n",
      "无痛胃肠镜检查前需进行麻醉会诊，评估麻醉风险，建议提前更改，以免造成不便。\n",
      "-------\n",
      "胃肠镜检查可以指定医生吗？\n",
      "\n",
      "胃肠镜检查的医生都是相关专业的专科医生，都是比较优秀的，请您放心。如您的体检时间和医生出诊时间符合，我们会尽量为您安排，但不能保证，如未能满足您的需求，请见谅！\n",
      "-------\n",
      "胃肠镜检查为什么要预存？\n",
      "\n",
      "体检套餐里的是胃肠镜的检查费，后续的息肉治疗等费用是需要另外收取的。预存进去\n",
      "\n",
      "主要是用于后续的胃肠镜息肉等治疗所产生的费用，多退少补。\n",
      "-------\n",
      "为什么预存余额没有全部退回？\n",
      "\n",
      "预存的钱是用于胃肠镜息肉摘除等后续的治疗费，治疗后可进行结算，多退少补。\n",
      "-------\n",
      "退回预存如何操作？\n",
      "\n",
      "退费是原路返回。（例如：支付宝支付是退回缴费的支付宝账户）\n",
      "\n",
      "请问您用的是什么方式进行的预存？\n",
      "\n",
      "①现金退款：凭体检关联卡、本人及代办人有效身份证件至一号楼南大厅收费处退现金。\n",
      "\n",
      "②支付宝/微信退款：凭体检关联卡、本人有效身份证件或者开卡时预留的手机号验证，至体检收费窗口原路退回。也可用预存时的手机关注温医一院公众号，绑定关联卡，直接手机退款。\n",
      "\n",
      "③银行卡退款：凭体检关联卡、银行卡、该银行卡持卡人的身份证至自助机操作退款。\n",
      "-------\n",
      "体检报告如何查询?\n",
      "\n",
      "请关注温医一院医疗保健中心微信公众号-就诊服务-体检报告查询\n",
      "\n",
      "或关注“温医一院”公众号--我的-体检报告\n",
      "-------\n",
      "体检报告多久出来?\n",
      "\n",
      "体检报告册一般体检后5-7个工作日生成。目前您的体检报告正在快马加鞭审核中，完成后将第一时间短信通知您。单项检查结果请关注请关注温医一院医疗保健中心微信公众号-就诊服务-单项结果查询或或关注“温医一院”公众号--我的-报告查询\n",
      "-------\n",
      "如何预约门诊及后续治疗？\n",
      "\n",
      "如需门诊就诊，请关注温医一院医疗保健中心微信公众号-就诊服务-门诊预约\n",
      "\n",
      "或关注“温医一院”公众号--就医-进行门诊预约\n",
      "-------\n",
      "职工体检可以来南白象院区吗？\n",
      "\n",
      "抱歉，暂不可以，职工体检根据相关规定可以去老院体检中心，或者去门诊体检。\n",
      "-------\n",
      "是否有做入学肺结核检查？\n",
      "\n",
      "我院医疗保健中心目前暂不提供入学肺结核检查。\n",
      "-------\n",
      "是否可以做驾驶证换证体检，直接12123上传？\n",
      "\n",
      "我院医疗保健中心目前暂不提供驾驶证换证体检。\n",
      "-------\n",
      "体检单位抬头发票可否改个人抬头？\n",
      "\n",
      "发票抬头可以到医疗保健中心5号楼1楼预约中心进行更改。\n",
      "-------\n",
      "鹿城区卫健局套餐能否变更？\n",
      "\n",
      "基础项目内容不能变更。可增加自费项目。\n",
      "-------\n",
      "单位体检能否变更检查项目？\n",
      "\n",
      "基础项目内容不能变更。自费项目可更改。\n",
      "-------\n",
      "公务员体检有没有胸部CT检查？\n",
      "\n",
      "公务员体检套餐中40周岁以上者有胸部CT项目，40周岁者以下没有胸部CT项目。\n",
      "-------\n",
      "有心梗史能体检吗？\n",
      "\n",
      "如果目前处于急性心梗期，请先就诊治疗，待病情稳定进行体检。既往心梗如有服用抗血小板凝集药物（如阿司匹林，波立维，华法令等）暂不能行胃肠镜检查。\n",
      "-------\n",
      "单位体检电话里能更改项目吗？\n",
      "\n",
      "目前不支持电话更改项目服务。请体检当天至预约中心办理。\n",
      "-------\n",
      "体检有未完成检查项目，如何预约？\n",
      "\n",
      "抱歉，无法提供线上预约，请拨打人工客服电话55578888/55578800咨询。\n",
      "-------\n",
      "胶囊内镜等体检项目怎么改约时间？\n",
      "\n",
      "请拨打人工客服电话55578888/55578800进行改约。\n",
      "-------\n",
      "入职体检，如果项目指标不正常，复查后正常医生会在入职表格上写合格，盖章吗？\n",
      "\n",
      "须专业医生评估后方能确定，请来院盖章时现场咨询医生。\n",
      "-------\n",
      "停车被收费怎么退费？\n",
      "\n",
      "请拨打人工客服电话55578888/55578800进行处理\n",
      "-------\n",
      "熬成的中药保质期多久？\n",
      "\n",
      "煎好并已包装的中药一般冷藏下可保存一个月。具体事项可咨询中药房，电话:55579583。\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\\n\\n\"],\n",
    "    chunk_size=0, # chunk_size=0, 它会严格按照separators进行分割\n",
    "    chunk_overlap=0,  # 思考：为什么要做overlap\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content.strip())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "LangChain 的 PDFLoader 和 TextSplitter 实现都比较粗糙，实际生产中不建议使用。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3、内置的 RAG 实现 \n",
    "\n",
    "- [检索型问答（Retrieval QA）](https://python.langchain.com.cn/docs/modules/chains/popular/vector_db_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 加载文档\n",
    "loader = Docx2txtLoader(\"体检中心问答.docx\")\n",
    "doc = loader.load()\n",
    "content = doc[0].page_content\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\\n\"],\n",
    "    chunk_size=0,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# 灌库\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_langchain_db\",  # 演示代码使用本地存储\n",
    ")\n",
    "\n",
    "# 演示需要，先重置向量数据库\n",
    "vectorstore.reset_collection()\n",
    "# 添加知识库\n",
    "vectorstore.add_documents(documents=docs)\n",
    "\n",
    "# LangChain内置的 RAG 实现\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    retriever=vectorstore.as_retriever(\n",
    "#         search_type=\"similarity\",\n",
    "#         search_kwargs={'k': 2}\n",
    "    ),\n",
    "    return_source_documents=True  # 返回参考的知识\n",
    ")\n",
    "\n",
    "query = \"空腹可以体检吗\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "print(\"======response=======\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4、小结\n",
    "\n",
    "1. 这部分能力 LangChain 的实现非常粗糙；\n",
    "2. 实际生产中，建议自己实现，不建议用 LangChain 的工具。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 三、记忆封装：Memory\n",
    "\n",
    "### 3.1、对话上下文：ConversationBufferMemory\n",
    "\n",
    "- 历史消息不限长度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "# input和output配对使用\n",
    "history.save_context({\"input\": \"你好啊\"}, {\"output\": \"你也好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"你再好啊\"}, {\"output\": \"你又好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2、只保留固定长度窗口的上下文：ConversationBufferWindowMemory\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"第一轮问\"}, {\"output\": \"第一轮答\"})\n",
    "window.save_context({\"input\": \"第二轮问\"}, {\"output\": \"第二轮答\"})\n",
    "window.save_context({\"input\": \"第三轮问\"}, {\"output\": \"第三轮答\"})\n",
    "print(window.load_memory_variables({}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 根据 Token 数限定 Memory 大小 ConversationTokenBufferMemory\n",
    "\n",
    "- https://python.langchain.com/docs/modules/memory/types/token_buffer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm = ChatOpenAI(),\n",
    "    max_token_limit=30\n",
    ")\n",
    "\n",
    "memory.save_context({\"input\": \"你好啊\"}, {\"output\": \"你好，我是你的AI助手\"})\n",
    "memory.save_context({\"input\": \"你会干什么\"}, {\"output\": \"我什么都会\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4、自动对历史信息做摘要：ConversationSummaryMemory\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    # buffer=\"The conversation is between a customer and a sales.\"\n",
    "    buffer=\"以中文表示\"\n",
    ")\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"你好，我是你的AI助手。我能为你回答有关AGIClass的各种问题。\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5、更多类型\n",
    "  \n",
    "- VectorStoreRetrieverMemory: 将 Memory 存储在向量数据库中，根据用户输入检索回最相关的部分\n",
    "  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6、小结\n",
    "\n",
    "1. LangChain 的 Memory 管理机制属于可用的部分，尤其是简单情况如按轮数或按 Token 数管理；\n",
    "2. 对于复杂情况，它不一定是最优的实现，例如检索向量库方式，建议根据实际情况和效果评估。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 四、LangChain Expression Language (LCEL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LangChain Expression Language（LCEL）是一种声明式语言，可轻松组合不同的调用顺序构成 Chain。\n",
    "\n",
    "<img src=\"langchain.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "LCEL的一些亮点包括：\n",
    "\n",
    "1. **流支持**：使用 LCEL 构建 Chain 时，你可以获得最佳的首个令牌时间（即从输出开始到首批输出生成的时间）。对于某些 Chain，这意味着可以直接从LLM流式传输令牌到流输出解析器，从而以与 LLM 提供商输出原始令牌相同的速率获得解析后的、增量的输出。\n",
    "\n",
    "2. **异步支持**：任何使用 LCEL 构建的链条都可以通过同步API（例如，在 Jupyter 笔记本中进行原型设计时）和异步 API（例如，在 LangServe 服务器中）调用。这使得相同的代码可用于原型设计和生产环境，具有出色的性能，并能够在同一服务器中处理多个并发请求。\n",
    "\n",
    "3. **优化的并行执行**：当你的 LCEL 链条有可以并行执行的步骤时（例如，从多个检索器中获取文档），我们会自动执行，无论是在同步还是异步接口中，以实现最小的延迟。\n",
    "\n",
    "4. **重试和回退**：为 LCEL 链的任何部分配置重试和回退。这是使链在规模上更可靠的绝佳方式。目前我们正在添加重试/回退的流媒体支持，因此你可以在不增加任何延迟成本的情况下获得增加的可靠性。\n",
    "\n",
    "5. **访问中间结果**：对于更复杂的链条，访问在最终输出产生之前的中间步骤的结果通常非常有用。这可以用于让最终用户知道正在发生一些事情，甚至仅用于调试链条。你可以流式传输中间结果，并且在每个LangServe服务器上都可用。\n",
    "\n",
    "6. **输入和输出模式**：输入和输出模式为每个 LCEL 链提供了从链的结构推断出的 Pydantic 和 JSONSchema 模式。这可以用于输入和输出的验证，是 LangServe 的一个组成部分。\n",
    "\n",
    "7. **无缝LangSmith跟踪集成**：随着链条变得越来越复杂，理解每一步发生了什么变得越来越重要。通过 LCEL，所有步骤都自动记录到 LangSmith，以实现最大的可观察性和可调试性。\n",
    "\n",
    "8. **无缝LangServe部署集成**：任何使用 LCEL 创建的链都可以轻松地使用 LangServe 进行部署。\n",
    "\n",
    "原文：https://python.langchain.com/v0.1/docs/expression_language/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 看个例子"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 向量数据库\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_langchain_db\",  # 演示代码使用本地存储\n",
    ")\n",
    "# 演示需要，先重置向量数据库\n",
    "vectorstore.reset_collection()\n",
    "\n",
    "# 加载文档\n",
    "loader = Docx2txtLoader(\"体检中心问答.docx\")\n",
    "doc = loader.load()\n",
    "content = doc[0].page_content\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\\n\"],\n",
    "    chunk_size=0,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# 添加知识库\n",
    "vectorstore.add_documents(documents=docs)\n",
    "\n",
    "# 检索接口\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt模板\n",
    "template = \"\"\"仅根据以下知识回答问题:\n",
    "{context}\n",
    "\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 模型\n",
    "model = OpenAI()\n",
    "\n",
    "\n",
    "# 用于打印检索到的知识\n",
    "class PrintContext(Runnable):\n",
    "    def invoke(self, inputs, config):\n",
    "        print(\"Context:\", inputs[\"context\"])\n",
    "        return inputs\n",
    "\n",
    "\n",
    "# LCEL 表达式\n",
    "retrieval_chain = (\n",
    "        {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "        | PrintContext()  # 打印检索到的知识\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = retrieval_chain.invoke(\"空腹可以体检嘛？\")\n",
    "\n",
    "print(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "更多的chain：https://python.langchain.com/v0.1/docs/modules/chains/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多的chain：https://python.langchain.com/v0.1/docs/modules/chains/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么使用LCEL？\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/expression_language/why/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 LCEL，还可以实现\n",
    "\n",
    "1. 配置运行时变量：https://python.langchain.com/docs/expression_language/how_to/configure\n",
    "2. 故障回退：https://python.langchain.com/docs/expression_language/how_to/fallbacks\n",
    "3. 并行调用：https://python.langchain.com/docs/expression_language/how_to/map\n",
    "4. 逻辑分支：https://python.langchain.com/docs/expression_language/how_to/routing\n",
    "5. 调用自定义流式函数：https://python.langchain.com/docs/expression_language/how_to/generators\n",
    "6. 链接外部Memory：https://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "\n",
    "更多例子：https://python.langchain.com/docs/expression_language/cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、智能体架构：Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 回忆：什么是智能体（Agent）\n",
    "\n",
    "将大语言模型作为一个推理引擎。给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。\n",
    "\n",
    "<img src=\"agent-overview.png\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "\n",
    "Agent的类型有很多，详见文档：https://python.langchain.com/v0.1/docs/modules/agents/agent_types/ ， 下面介绍两种智能体：ReAct 和 Self Ask With Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 先定义一些工具：Tools\n",
    "\n",
    "- 可以是一个函数或三方 API：https://python.langchain.com/v0.2/docs/integrations/tools/\n",
    "\n",
    "- 也可以把一个 Chain 或者 Agent 的 run()作为一个 Tool\n",
    "\n",
    "- 下面代码使用到SerpAPI搜索工具，使用前需要配置好`SERPAPI_API_KEY`去这里申请：https://serpapi.com/search-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool, tool\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "\n",
    "# 使用SerpAPI搜索工具，搜索引擎替换为Baidu（默认是google）\n",
    "# params = {\n",
    "#   \"engine\": \"Baidu\"\n",
    "# }\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "# 自定义工具\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    # llm会利用下面的注释来识别函数的功能\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "    weekday\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 智能体类型：ReAct （Reasoning + Acting）\n",
    "\n",
    "- LangChain文档：https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/\n",
    "\n",
    "- ReAct论文：https://react-lm.github.io/ \n",
    "\n",
    "- 使用前先安装langchainhub， `!pip install langchainhub`。\n",
    "\n",
    "- 需要配置好`LANGCHAIN_API_KEY` 或 `LANGCHAIN_HUB_API_KEY`，去这里申请：https://smith.langchain.com/hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=600px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========prompt模板===========\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_27780\\2666564159.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mllm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOpenAI\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# 定义一个agent：需要大模型、工具集和prompt模板\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[0magent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_react_agent\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mllm\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mllm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtools\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtools\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprompt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprompt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;31m# 定义一个执行器：需要agent对象和工具集，verbose=True 会打印中间过程\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[0magent_exector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAgentExecutor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtools\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtools\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# 下载一个现有的prompt模板\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(\"=========prompt模板===========\")\n",
    "print(prompt.template)\n",
    "\n",
    "llm = OpenAI()\n",
    "# 定义一个agent：需要大模型、工具集和prompt模板\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "# 定义一个执行器：需要agent对象和工具集，verbose=True 会打印中间过程\n",
    "agent_exector = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "# 执行\n",
    "print(\"=========执行过程===========\")\n",
    "agent_exector.invoke({\"input\": \"周杰伦生日那天是星期几\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 智能体类型：Self-Ask With Search\n",
    "\n",
    "- 这是一种自问自答形式的agent\n",
    "\n",
    "- LangChain文档：https://python.langchain.com/v0.1/docs/modules/agents/agent_types/self_ask_with_search/\n",
    "\n",
    "- self_ask_with_search_agent 只能传一个名为‘Intermediate Answer’的tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========prompt模板===========\n",
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output:  Yes.\nFollow up: Who is 吴京?\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutputParserException\u001B[0m                     Traceback (most recent call last)",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_iter_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1341\u001B[0m             \u001B[1;31m# Call the LLM to see what to do.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1342\u001B[1;33m             output = self.agent.plan(\n\u001B[0m\u001B[0;32m   1343\u001B[0m                 \u001B[0mintermediate_steps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36mplan\u001B[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    460\u001B[0m             \u001B[1;31m# accumulate the output into final output and return that.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 461\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunnable\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"callbacks\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    462\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mfinal_output\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mstream\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3261\u001B[0m     ) -> Iterator[Output]:\n\u001B[1;32m-> 3262\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3263\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3248\u001B[0m     ) -> Iterator[Output]:\n\u001B[1;32m-> 3249\u001B[1;33m         yield from self._transform_stream_with_config(\n\u001B[0m\u001B[0;32m   3250\u001B[0m             \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36m_transform_stream_with_config\u001B[1;34m(self, input, transformer, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   2053\u001B[0m                 \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2054\u001B[1;33m                     \u001B[0mchunk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2055\u001B[0m                     \u001B[1;32myield\u001B[0m \u001B[0mchunk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36m_transform\u001B[1;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[0;32m   3210\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3211\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfinal_pipeline\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3212\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   1289\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mgot_first_val\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1290\u001B[1;33m             \u001B[1;32myield\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1291\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mstream\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    854\u001B[0m         \"\"\"\n\u001B[1;32m--> 855\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    856\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\output_parsers\\base.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, input, config)\u001B[0m\n\u001B[0;32m    191\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 192\u001B[1;33m             return self._call_with_config(\n\u001B[0m\u001B[0;32m    193\u001B[0m                 \u001B[1;32mlambda\u001B[0m \u001B[0minner_input\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mGeneration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minner_input\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36m_call_with_config\u001B[1;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   1784\u001B[0m                 \u001B[0mOutput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1785\u001B[1;33m                 context.run(\n\u001B[0m\u001B[0;32m   1786\u001B[0m                     \u001B[0mcall_func_with_variable_args\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# type: ignore[arg-type]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\config.py\u001B[0m in \u001B[0;36mcall_func_with_variable_args\u001B[1;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    426\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"run_manager\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 427\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    428\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\output_parsers\\base.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(inner_input)\u001B[0m\n\u001B[0;32m    192\u001B[0m             return self._call_with_config(\n\u001B[1;32m--> 193\u001B[1;33m                 \u001B[1;32mlambda\u001B[0m \u001B[0minner_input\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mGeneration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minner_input\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    194\u001B[0m                 \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\output_parsers\\base.py\u001B[0m in \u001B[0;36mparse_result\u001B[1;34m(self, result, partial)\u001B[0m\n\u001B[0;32m    236\u001B[0m         \"\"\"\n\u001B[1;32m--> 237\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    238\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\output_parsers\\self_ask.py\u001B[0m in \u001B[0;36mparse\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinish_string\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlast_line\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mOutputParserException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Could not parse output: {text}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mAgentFinish\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"output\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlast_line\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinish_string\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOutputParserException\u001B[0m: Could not parse output:  Yes.\nFollow up: Who is 吴京?\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_27780\\1969460717.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[0magent_executor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAgentExecutor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtools\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtools\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m \u001B[0magent_executor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"input\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"吴京的老婆的主持过哪些节目\"\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    162\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 164\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    165\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    152\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m             outputs = (\n\u001B[1;32m--> 154\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    155\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m                 \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m   1606\u001B[0m         \u001B[1;31m# We now enter the agent loop (until it returns something).\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1607\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_should_continue\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterations\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime_elapsed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1608\u001B[1;33m             next_step_output = self._take_next_step(\n\u001B[0m\u001B[0;32m   1609\u001B[0m                 \u001B[0mname_to_tool_map\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1610\u001B[0m                 \u001B[0mcolor_mapping\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_take_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1312\u001B[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001B[0;32m   1313\u001B[0m         return self._consume_next_step(\n\u001B[1;32m-> 1314\u001B[1;33m             [\n\u001B[0m\u001B[0;32m   1315\u001B[0m                 \u001B[0ma\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m                 for a in self._iter_next_step(\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1312\u001B[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001B[0;32m   1313\u001B[0m         return self._consume_next_step(\n\u001B[1;32m-> 1314\u001B[1;33m             [\n\u001B[0m\u001B[0;32m   1315\u001B[0m                 \u001B[0ma\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m                 for a in self._iter_next_step(\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_iter_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1351\u001B[0m                 \u001B[0mraise_error\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1352\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mraise_error\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1353\u001B[1;33m                 raise ValueError(\n\u001B[0m\u001B[0;32m   1354\u001B[0m                     \u001B[1;34m\"An output parsing error occurred. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1355\u001B[0m                     \u001B[1;34m\"In order to pass this error back to the agent and have it try \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output:  Yes.\nFollow up: Who is 吴京?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilyAnswer\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_self_ask_with_search_agent\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# 使用SerpAPI搜索工具，搜索引擎替换为Baidu（默认是google）\n",
    "params = {\n",
    "  \"engine\": \"Baidu\"\n",
    "}\n",
    "search = SerpAPIWrapper(params=params)\n",
    "\n",
    "# self_ask_with_search_agent 只能传一个名为‘Intermediate Answer’的tool\n",
    "tools = [\n",
    "    # TavilyAnswer(max_results=1, name=\"Intermediate Answer\"),\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "print(\"=========prompt模板===========\")\n",
    "print(prompt.template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"吴京的老婆的主持过哪些节目\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 OpenAI Assistants\n",
    "\n",
    "支持OpenAI的Assistants API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\(10\\) 减去 \\(4\\) 的差的 \\(2.3\\) 次方是约 \\(61.624\\)。\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\n",
    "    name=\"langchain assistant\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-1106-preview\",\n",
    ")\n",
    "output = interpreter_assistant.invoke({\"content\": \"10减4的差的2.3次方是多少\"})\n",
    "\n",
    "print(output[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "<ol>\n",
    "<li>ReAct 是比较常用的 智能体</li>\n",
    "<li>SelfAskWithSearch 更适合需要层层推理的场景（例如知识图谱）</li>\n",
    "<li>OpenAI Assistants 不是万能的，后面课程中我们会专门讲 Agent 的实现</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、LangServe\n",
    "\n",
    "LangServe 用于将 Chain 或者 Runnable 部署成一个 REST API 服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 LangServe\n",
    "!pip install \"langserve[all]\"\n",
    "\n",
    "# 也可以只安装一端\n",
    "# !pip install \"langserve[client]\"\n",
    "# !pip install \"langserve[server]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1、Server端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"讲一个关于{topic}的笑话\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8080)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2、Client端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "joke_chain = RemoteRunnable(\"http://localhost:8080/joke/\")\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"小明\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "1. LangChain 随着版本迭代可用性有明显提升\n",
    "2. 使用 LangChain 要避开存在大量代码内 Prompt 的模块\n",
    "3. 它的内置基础工具，建议充分测试效果后再决定是否使用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}