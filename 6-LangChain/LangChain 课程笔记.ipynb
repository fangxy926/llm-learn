{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写在前面\n",
    "\n",
    "- LangChain 也是一套面向大模型的开发框架（SDK）\n",
    "- LangChain 并不完美，还在不断迭代中\n",
    "- 学习 Langchain 更重要的是借鉴其思想，具体的接口和模块可能很快就会改变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain vs. Semantic Kernel\n",
    "\n",
    "[![Star History Chart](https://api.star-history.com/svg?repos=langchain-ai/langchain,microsoft/semantic-kernel&type=Date)](https://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&Date)\n",
    "\n",
    "\n",
    "LangChain 完胜？我们接下来仔细看看。\n",
    "\n",
    "\n",
    "| 功能/工具           | LangChain                       | Semantic Kernel                  |\n",
    "|-------------------|:---------------------------------:|:----------------------------------:|\n",
    "| 版本号        |  0.1.0  | python-0.4.4.dev  |\n",
    "| 适配的 LLM        | 多   | 少 + 外部生态   |\n",
    "| Prompt 工具        | 支持    | 支持     |\n",
    "| Prompt 函数嵌套    | 需要通过 LCEL | 支持        |\n",
    "| Prompt 模板嵌套    | 不支持  | 不支持       |\n",
    "| 输出解析工具       | 支持  | 不支持  |\n",
    "| 上下文管理工具           | 支持 | C#版支持，Python版尚未支持  |\n",
    "| 内置工具           | 多，但良莠不齐  | 少 + 外部生态  |\n",
    "| 三方向量数据库适配           | 多 | 少 + 外部生态  |\n",
    "| 服务部署 | LangServe | 与 Azure 衔接更丝滑\n",
    "| 管理工具 | LangSmith/LangFuse | Prompt Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangChain 的核心组件\n",
    "\n",
    "<img src=\"langchain_stack.png\" style=\"margin-left: 0px\" width=1000px>\n",
    "\n",
    "1. 模型 I/O 封装\n",
    "   - LLMs：大语言模型\n",
    "   - Chat Models：一般基于 LLMs，但按对话结构重新封装\n",
    "   - PromptTemple：提示词模板\n",
    "   - OutputParser：解析输出\n",
    "2. 数据连接封装\n",
    "   - Document Loaders：各种格式文件的加载器\n",
    "   - Document Transformers：对文档的常用操作，如：split, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Models：文本向量化表示，用于检索等操作（啥意思？别急，后面详细讲）\n",
    "   - Verctorstores: （面向检索的）向量的存储\n",
    "   - Retrievers: 向量的检索\n",
    "3. 记忆封装\n",
    "   - Memory：这里不是物理内存，从文本的角度，可以理解为“上文”、“历史记录”或者说“记忆力”的管理\n",
    "4. 架构封装\n",
    "\n",
    "   - Chain：实现一个功能或者一系列顺序功能组合\n",
    "\n",
    "   - Agent：根据用户输入，自动规划执行步骤，自动选择每步需要的工具，最终完成用户指定的功能\n",
    "     - Tools：调用外部功能的函数，例如：调 google 搜索、文件 I/O、Linux Shell 等等\n",
    "     - Toolkits：操作某软件的一组工具集，例如：操作 DB、操作 Gmail 等等\n",
    "\n",
    "   - Chain及Agent架构设计（下图为ReAct类型智能体，后文会介绍）\n",
    "   \n",
    "   <img src=\"langchain.png\" style=\"margin-left: 0px\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、模型 I/O 封装\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/model_io/\n",
    "\n",
    "\n",
    "把不同的模型，统一封装成一个接口，方便更换模型而不用重构代码。\n",
    "\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "在Model I/O这一流程中，LangChain抽象的组件主要有三个：\n",
    "\n",
    "- Language models\n",
    "- Prompts\n",
    "- Output parsers\n",
    "\n",
    "\n",
    "\n",
    "安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#安装最新版本\n",
    "!pip install langchain==0.2.13\n",
    "!pip install langchain-openai\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 模型封装：LLM vs. ChatModel\n",
    "\n",
    "LangChain中提供了三类模型的封装，LLM、ChatModel和Embedding，下面聊一下LLM和ChatModel的区别：\n",
    "\n",
    "（1）LLM：输入和输出都是**纯文本**的模型（text in， text out）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n我是一个人工智能助手，被设计来回答问题、提供帮助和进行对话。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "text = \"你是谁？\"\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）ChatModel：是LLM的变体，对聊天场景进行了抽象。输入不是纯文本，而是chat message列表，输出也是chat message。\n",
    "\n",
    "    chat message = text + 消息类型(System, Human, AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你是张三。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 26, 'total_tokens': 31}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-25c56638-f0f4-4a8c-af2e-62149d92712f-0', usage_metadata={'input_tokens': 26, 'output_tokens': 5, 'total_tokens': 31})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "chat_model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chat message目前一共有5种类型，每种message至少包含角色（role）和内容（content）两个参数：\n",
    "\n",
    "    - HumanMessage：等价于OpenAI接口中的user role\n",
    "\n",
    "    - AIMessage：等价于OpenAI接口中的assistant role\n",
    "    \n",
    "    - SystemMessage：等价于OpenAI接口中的system role\n",
    "\n",
    "    - FunctionMessage：function call的结果。除了role和content之外，它还有一个name参数，表示函数的名称。\n",
    "    \n",
    "    - ToolMessage：tool call的结果。除了role和content之外，它还有一个tool_call_id参数。\n",
    "\n",
    "    详细文档：https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您是张三，一位学员。您可以告诉我您有什么问题或者需要帮助的地方。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 51, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bcb315c1-bf88-473f-b855-182c3bfe5b15-0', usage_metadata={'input_tokens': 51, 'output_tokens': 33, 'total_tokens': 84})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是AGIClass的课程助理。\"), \n",
    "    HumanMessage(content=\"我是学员，我叫张三\"),\n",
    "    AIMessage(content=\"欢迎！\"),\n",
    "    HumanMessage(content=\"我是谁？\")\n",
    "]\n",
    "\n",
    "chat_model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "（3）**为什么要区分LLM和ChatModel？**\n",
    "\n",
    "- LLM主要处理一次性问题，适用于回答问题、生成文本等场景\n",
    "- Chat Model 则专注于多轮对话场景，不仅接受用户的输入，还考虑对话的上下文\n",
    "\n",
    "\n",
    "（4）通过模型封装，可以实现不同模型的统一接口调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 从langchain_community导入百度千帆大模型\n",
    "from langchain_community.chat_models import ErnieBotChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "ernie = ErnieBotChat()\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"你是谁\") \n",
    "]\n",
    "\n",
    "ernie.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt模板封装\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
    "\n",
    "#### 1.2.1 PromptTemplate\n",
    "- PromptTemplate是纯字符串形式\n",
    "- 使用类似于str.format()语法进行参数补全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject']\n",
      "给我讲个关于小明的笑话\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# 参数用{}包裹\n",
    "template = PromptTemplate.from_template(\"给我讲个关于{subject}的笑话\")\n",
    "print(template.input_variables)\n",
    "print(template.format(subject='小明'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 ChatPromptTemplate\n",
    "- ChatPromptTemplate是对chat message的封装，携带角色信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是AGI课堂的客服助手。你的名字叫瓜瓜'), HumanMessage(content='你是谁')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 等价于 {\"role\": \"system\", \"content\": \"你是{product}的客服助手。你的名字叫{name}\"},\n",
    "        SystemMessagePromptTemplate.from_template(\"你是{product}的客服助手。你的名字叫{name}\"),\n",
    "        # 等价于 {\"role\": \"user\", \"content\": \"{query}\"},\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "        product=\"AGI课堂\",\n",
    "        name=\"瓜瓜\",\n",
    "        query=\"你是谁\"\n",
    "    )\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 从文件加载Prompt模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Yaml格式\n",
    "\n",
    "``` yml\n",
    " _type: prompt\n",
    "input_variables:\n",
    "    [\"adjective\", \"content\"]\n",
    "template: \n",
    "    Tell me a {adjective} joke about {content}.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. JSON格式\n",
    "\n",
    "```JSON\n",
    "\n",
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template\": \"Tell me a {adjective} joke about {content}.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Template单独存放\n",
    "\n",
    "- 注意点：使用`template_path`\n",
    "- 从代码文件的相对路径找模板文件\n",
    "\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template_path\": \"simple_template.txt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about fox.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"simple_prompt.yaml\")\n",
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "\n",
    "print(prompt.format(adjective=\"funny\", content=\"fox\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 输出封装 OutputParser\n",
    "\n",
    "自动把 LLM 输出的字符串转换为指定格式\n",
    "\n",
    "LangChain 内置的 OutputParser 包括:\n",
    "\n",
    "- PydanticParser\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- XMLParser\n",
    "\n",
    "等等， 官方文档：https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/\n",
    "\n",
    "### 1.3.1 Pydantic (JSON) Parser\n",
    "\n",
    "- Pydantic 是一个用于数据验证和数据解析的 Python 库。\n",
    "- PydanticParser可以自动根据Pydantic类的定义，**生成JSON格式的输出**。\n",
    "- Pydantic v1版本即将弃用，建议使用v2版本：https://python.langchain.com/v0.2/docs/how_to/pydantic_compatibility/\n",
    "- 使用PydanticParser的好处，不需要在prompt中描述一大串json格式的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "# 定义你的输出对象\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Format Instruction=====\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"description\": \"Year\", \"title\": \"Year\", \"type\": \"integer\"}, \"month\": {\"description\": \"Month\", \"title\": \"Month\", \"type\": \"integer\"}, \"day\": {\"description\": \"Day\", \"title\": \"Day\", \"type\": \"integer\"}, \"era\": {\"description\": \"BC or AD\", \"title\": \"Era\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "text='提取用户输入中的日期。\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"year\": {\"description\": \"Year\", \"title\": \"Year\", \"type\": \"integer\"}, \"month\": {\"description\": \"Month\", \"title\": \"Month\", \"type\": \"integer\"}, \"day\": {\"description\": \"Day\", \"title\": \"Day\", \"type\": \"integer\"}, \"era\": {\"description\": \"BC or AD\", \"title\": \"Era\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\\n```\\n用户输入:\\n2023年四月6日天气晴...'\n",
      "====Output=====\n",
      "content='{\\n  \"year\": 2023,\\n  \"month\": 4,\\n  \"day\": 6,\\n  \"era\": \"AD\"\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 256, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-cc701941-4a42-4779-8157-e934b691ec8a-0' usage_metadata={'input_tokens': 256, 'output_tokens': 31, 'total_tokens': 287}\n",
      "====Parsed=====\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# 根据Pydantic对象的定义，构造一个OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "template = \"\"\"提取用户输入中的日期。\n",
    "{format_instructions}\n",
    "用户输入:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    # 预先赋值：直接从OutputParser中获取输出描述，并对模板的变量预先赋值\n",
    "    # https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()} \n",
    ")\n",
    "\n",
    "print(\"====Format Instruction=====\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "\n",
    "query = \"2023年四月6日天气晴...\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(model_input)\n",
    "\n",
    "output = model.invoke(model_input)\n",
    "print(\"====Output=====\")\n",
    "print(output)\n",
    "print(\"====Parsed=====\")\n",
    "date = parser.parse(output.content)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Auto-Fixing Parser\n",
    "\n",
    "利用LLM自动根据解析异常修复并重新解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===格式错误的Output===\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===出现异常===\n",
      "Invalid json output: {\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===重新解析结果===\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "#我们把之前output的格式改错\n",
    "output = output.content.replace(\"4\",\"四\")\n",
    "print(\"===格式错误的Output===\")\n",
    "print(output)\n",
    "try:\n",
    "    date = parser.parse(output)\n",
    "except Exception as e:\n",
    "    print(\"===出现异常===\")\n",
    "    print(e)\n",
    "    \n",
    "#用OutputFixingParser自动修复并解析\n",
    "date = new_parser.parse(output)\n",
    "print(\"===重新解析结果===\")\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "猜一下OutputFixingParser是怎么做到的？ 再调一遍大模型告诉他哪里不对，让它修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4、小结\n",
    "\n",
    "1. LangChain 统一封装了各种模型的调用接口\n",
    "2. LangChain 提供了提示词模板类，可以自定义带变量的模板\n",
    "3. LangChain 提供了一些输出解析器，用于将大模型的输出解析成结构化对象；额外带有自动修复功能。\n",
    "4. 上述模型属于 LangChain 中较为优秀的部分；美中不足的是 OutputParser 自身的 Prompt 维护在代码中，耦合度较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据连接封装\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "### 2.1 文档加载器：Document Loaders\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 加载PDF需要的\n",
    "!pip install pypdf\n",
    "# 加载docx需要的\n",
    "!pip install python-docx\n",
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"体检中心问答.docx\")\n",
    "doc = loader.load()\n",
    "content = doc[0].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "content[0:500]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 文档处理器 TextSplitter\n",
    "\n",
    "> 官方文档：https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\\n\\n\"],\n",
    "    chunk_size=0, # chunk_size=0, 它会严格按照separators进行分割\n",
    "    chunk_overlap=0,  # 思考：为什么要做overlap\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content.strip())\n",
    "    print('-------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "LangChain 的 PDFLoader 和 TextSplitter 实现都比较粗糙，实际生产中不建议使用。\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3、内置的 RAG 实现 \n",
    "\n",
    "- [检索型问答（Retrieval QA）](https://python.langchain.com.cn/docs/modules/chains/popular/vector_db_qa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install langchain-chroma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 加载文档\n",
    "loader = Docx2txtLoader(\"体检中心问答.docx\")\n",
    "doc = loader.load()\n",
    "content = doc[0].page_content\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\\n\"],\n",
    "    chunk_size=0,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# 灌库\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_langchain_db\",  # 演示代码使用本地存储\n",
    ")\n",
    "\n",
    "# 演示需要，先重置向量数据库\n",
    "vectorstore.reset_collection()\n",
    "# 添加知识库\n",
    "vectorstore.add_documents(documents=docs)\n",
    "\n",
    "# LangChain内置的 RAG 实现\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={'k': 2}\n",
    "    ),\n",
    "    return_source_documents=True  # 返回参考的知识\n",
    ")\n",
    "\n",
    "query = \"空腹可以体检吗\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "print(\"======response=======\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4、小结\n",
    "\n",
    "1. 这部分能力 LangChain 的实现非常粗糙；\n",
    "2. 实际生产中，建议自己实现，不建议用 LangChain 的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、记忆封装：Memory\n",
    "\n",
    "### 3.1、对话上下文：ConversationBufferMemory\n",
    "\n",
    "- 历史消息不限长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好啊\\nAI: 你也好啊'}\n",
      "{'history': 'Human: 你好啊\\nAI: 你也好啊\\nHuman: 你再好啊\\nAI: 你又好啊'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "# input和output配对使用\n",
    "history.save_context({\"input\": \"你好啊\"}, {\"output\": \"你也好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"你再好啊\"}, {\"output\": \"你又好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2、只保留固定长度窗口的上下文：ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 第二轮问\\nAI: 第二轮答\\nHuman: 第三轮问\\nAI: 第三轮答'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"第一轮问\"}, {\"output\": \"第一轮答\"})\n",
    "window.save_context({\"input\": \"第二轮问\"}, {\"output\": \"第二轮答\"})\n",
    "window.save_context({\"input\": \"第三轮问\"}, {\"output\": \"第三轮答\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 根据 Token 数限定 Memory 大小 ConversationTokenBufferMemory\n",
    "\n",
    "- https://python.langchain.com/docs/modules/memory/types/token_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: 我什么都会'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm = ChatOpenAI(),\n",
    "    max_token_limit=20\n",
    ")\n",
    "\n",
    "memory.save_context({\"input\": \"你好啊\"}, {\"output\": \"你好，我是你的AI助手\"})\n",
    "memory.save_context({\"input\": \"你会干什么\"}, {\"output\": \"我什么都会\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4、自动对历史信息做摘要：ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': '\\n人类问AI对人工智能的看法。AI认为人工智能是一种积极的力量，因为它能帮助人类发挥他们的全部潜力。人类向AI打招呼，AI回应说它是人类的AI助手，可以回答关于AGIClass的各种问题。'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    # buffer=\"The conversation is between a customer and a sales.\"\n",
    "    buffer=\"以中文表示\"\n",
    ")\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"你好，我是你的AI助手。我能为你回答有关AGIClass的各种问题。\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5、更多类型\n",
    "  \n",
    "- VectorStoreRetrieverMemory: 将 Memory 存储在向量数据库中，根据用户输入检索回最相关的部分\n",
    "  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6、小结\n",
    "\n",
    "1. LangChain 的 Memory 管理机制属于可用的部分，尤其是简单情况如按轮数或按 Token 数管理；\n",
    "2. 对于复杂情况，它不一定是最优的实现，例如检索向量库方式，建议根据实际情况和效果评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Language（LCEL）是一种声明式语言，可轻松组合不同的调用顺序构成 Chain。\n",
    "\n",
    "<img src=\"langchain.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "LCEL的一些亮点包括：\n",
    "\n",
    "1. **流支持**：使用 LCEL 构建 Chain 时，你可以获得最佳的首个令牌时间（即从输出开始到首批输出生成的时间）。对于某些 Chain，这意味着可以直接从LLM流式传输令牌到流输出解析器，从而以与 LLM 提供商输出原始令牌相同的速率获得解析后的、增量的输出。\n",
    "\n",
    "2. **异步支持**：任何使用 LCEL 构建的链条都可以通过同步API（例如，在 Jupyter 笔记本中进行原型设计时）和异步 API（例如，在 LangServe 服务器中）调用。这使得相同的代码可用于原型设计和生产环境，具有出色的性能，并能够在同一服务器中处理多个并发请求。\n",
    "\n",
    "3. **优化的并行执行**：当你的 LCEL 链条有可以并行执行的步骤时（例如，从多个检索器中获取文档），我们会自动执行，无论是在同步还是异步接口中，以实现最小的延迟。\n",
    "\n",
    "4. **重试和回退**：为 LCEL 链的任何部分配置重试和回退。这是使链在规模上更可靠的绝佳方式。目前我们正在添加重试/回退的流媒体支持，因此你可以在不增加任何延迟成本的情况下获得增加的可靠性。\n",
    "\n",
    "5. **访问中间结果**：对于更复杂的链条，访问在最终输出产生之前的中间步骤的结果通常非常有用。这可以用于让最终用户知道正在发生一些事情，甚至仅用于调试链条。你可以流式传输中间结果，并且在每个LangServe服务器上都可用。\n",
    "\n",
    "6. **输入和输出模式**：输入和输出模式为每个 LCEL 链提供了从链的结构推断出的 Pydantic 和 JSONSchema 模式。这可以用于输入和输出的验证，是 LangServe 的一个组成部分。\n",
    "\n",
    "7. **无缝LangSmith跟踪集成**：随着链条变得越来越复杂，理解每一步发生了什么变得越来越重要。通过 LCEL，所有步骤都自动记录到 LangSmith，以实现最大的可观察性和可调试性。\n",
    "\n",
    "8. **无缝LangServe部署集成**：任何使用 LCEL 创建的链都可以轻松地使用 LangServe 进行部署。\n",
    "\n",
    "原文：https://python.langchain.com/v0.1/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sam Altman\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 向量数据库\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_langchain_db\",  # 演示代码使用本地存储\n",
    ")\n",
    "# 演示需要，先重置向量数据库\n",
    "vectorstore.reset_collection()\n",
    "\n",
    "# 添加数据\n",
    "vectorstore.add_texts([\"Sam Altman是OpenAI的CEO\", \"Sam Altman被解雇了\", \"Sam Altman被复职了\"])\n",
    "\n",
    "# 检索接口\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt模板\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 模型\n",
    "model = OpenAI()\n",
    "\n",
    "# LCEL 表达式\n",
    "retrieval_chain = (\n",
    "        {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = retrieval_chain.invoke(\"OpenAI的CEO是谁\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么使用LCEL？\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/expression_language/why/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 LCEL，还可以实现\n",
    "\n",
    "1. 配置运行时变量：https://python.langchain.com/docs/expression_language/how_to/configure\n",
    "2. 故障回退：https://python.langchain.com/docs/expression_language/how_to/fallbacks\n",
    "3. 并行调用：https://python.langchain.com/docs/expression_language/how_to/map\n",
    "4. 逻辑分支：https://python.langchain.com/docs/expression_language/how_to/routing\n",
    "5. 调用自定义流式函数：https://python.langchain.com/docs/expression_language/how_to/generators\n",
    "6. 链接外部Memory：https://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "\n",
    "更多例子：https://python.langchain.com/docs/expression_language/cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、智能体架构：Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 回忆：什么是智能体（Agent）\n",
    "\n",
    "将大语言模型作为一个推理引擎。给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。\n",
    "\n",
    "<img src=\"agent-overview.png\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "\n",
    "Agent的类型有很多，详见文档：https://python.langchain.com/v0.1/docs/modules/agents/agent_types/ ， 下面介绍两种智能体：ReAct 和 Self Ask With Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 先定义一些工具：Tools\n",
    "\n",
    "- 可以是一个函数或三方 API：https://python.langchain.com/v0.2/docs/integrations/tools/\n",
    "\n",
    "- 也可以把一个 Chain 或者 Agent 的 run()作为一个 Tool\n",
    "\n",
    "- 下面代码使用到SerpAPI搜索工具，使用前需要配置好`SERPAPI_API_KEY`去这里申请：https://serpapi.com/search-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool, tool\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "\n",
    "# 使用SerpAPI搜索工具，搜索引擎替换为Baidu（默认是google）\n",
    "# params = {\n",
    "#   \"engine\": \"Baidu\"\n",
    "# }\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "# 自定义工具\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    # llm会利用下面的注释来识别函数的功能\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "    weekday\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 智能体类型：ReAct （Reasoning + Acting）\n",
    "\n",
    "- LangChain文档：https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/\n",
    "\n",
    "- ReAct论文：https://react-lm.github.io/ \n",
    "\n",
    "- 使用前先安装langchainhub， `!pip install langchainhub`。\n",
    "\n",
    "- 需要配置好`LANGCHAIN_API_KEY` 或 `LANGCHAIN_HUB_API_KEY`，去这里申请：https://smith.langchain.com/hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=600px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========prompt模板===========\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n",
      "=========执行过程===========\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I need to know what day of the week it is on Jay Chou's birthday\n",
      "Action: weekday\n",
      "Action Input: '01/18/2021'\n",
      "\u001B[0m\u001B[33;1m\u001B[1;3mMonday\u001B[0m\u001B[32;1m\u001B[1;3m I now know the day of the week Jay Chou's birthday is on\n",
      "Final Answer: Monday\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '周杰伦生日那天是星期几', 'output': 'Monday'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# 下载一个现有的prompt模板\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(\"=========prompt模板===========\")\n",
    "print(prompt.template)\n",
    "\n",
    "llm = OpenAI()\n",
    "# 定义一个agent：需要大模型、工具集和prompt模板\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "# 定义一个执行器：需要agent对象和工具集，verbose=True 会打印中间过程\n",
    "agent_exector = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "# 执行\n",
    "print(\"=========执行过程===========\")\n",
    "agent_exector.invoke({\"input\": \"周杰伦生日那天是星期几\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 智能体类型：Self-Ask With Search\n",
    "\n",
    "- 这是一种自问自答形式的agent\n",
    "\n",
    "- LangChain文档：https://python.langchain.com/v0.1/docs/modules/agents/agent_types/self_ask_with_search/\n",
    "\n",
    "- self_ask_with_search_agent 只能传一个名为‘Intermediate Answer’的tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========prompt模板===========\n",
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output:  Yes.\nFollowup: 谁是吴京的老婆?\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutputParserException\u001B[0m                     Traceback (most recent call last)",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_iter_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1341\u001B[0m             \u001B[1;31m# Call the LLM to see what to do.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1342\u001B[1;33m             output = self.agent.plan(\n\u001B[0m\u001B[0;32m   1343\u001B[0m                 \u001B[0mintermediate_steps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36mplan\u001B[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    460\u001B[0m             \u001B[1;31m# accumulate the output into final output and return that.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 461\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunnable\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"callbacks\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    462\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mfinal_output\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mstream\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3261\u001B[0m     ) -> Iterator[Output]:\n\u001B[1;32m-> 3262\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3263\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   3248\u001B[0m     ) -> Iterator[Output]:\n\u001B[1;32m-> 3249\u001B[1;33m         yield from self._transform_stream_with_config(\n\u001B[0m\u001B[0;32m   3250\u001B[0m             \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36m_transform_stream_with_config\u001B[1;34m(self, input, transformer, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   2053\u001B[0m                 \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2054\u001B[1;33m                     \u001B[0mchunk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2055\u001B[0m                     \u001B[1;32myield\u001B[0m \u001B[0mchunk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36m_transform\u001B[1;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[0;32m   3210\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3211\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfinal_pipeline\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3212\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   1289\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mgot_first_val\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1290\u001B[1;33m             \u001B[1;32myield\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1291\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36mstream\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    854\u001B[0m         \"\"\"\n\u001B[1;32m--> 855\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    856\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\output_parsers\\base.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, input, config)\u001B[0m\n\u001B[0;32m    191\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 192\u001B[1;33m             return self._call_with_config(\n\u001B[0m\u001B[0;32m    193\u001B[0m                 \u001B[1;32mlambda\u001B[0m \u001B[0minner_input\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mGeneration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minner_input\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\base.py\u001B[0m in \u001B[0;36m_call_with_config\u001B[1;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   1784\u001B[0m                 \u001B[0mOutput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1785\u001B[1;33m                 context.run(\n\u001B[0m\u001B[0;32m   1786\u001B[0m                     \u001B[0mcall_func_with_variable_args\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# type: ignore[arg-type]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\runnables\\config.py\u001B[0m in \u001B[0;36mcall_func_with_variable_args\u001B[1;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    426\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"run_manager\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 427\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    428\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\output_parsers\\base.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(inner_input)\u001B[0m\n\u001B[0;32m    192\u001B[0m             return self._call_with_config(\n\u001B[1;32m--> 193\u001B[1;33m                 \u001B[1;32mlambda\u001B[0m \u001B[0minner_input\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mGeneration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minner_input\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    194\u001B[0m                 \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain_core\\output_parsers\\base.py\u001B[0m in \u001B[0;36mparse_result\u001B[1;34m(self, result, partial)\u001B[0m\n\u001B[0;32m    236\u001B[0m         \"\"\"\n\u001B[1;32m--> 237\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    238\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\output_parsers\\self_ask.py\u001B[0m in \u001B[0;36mparse\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinish_string\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlast_line\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mOutputParserException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Could not parse output: {text}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mAgentFinish\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"output\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlast_line\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinish_string\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOutputParserException\u001B[0m: Could not parse output:  Yes.\nFollowup: 谁是吴京的老婆?\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_47016\\1969460717.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[0magent_executor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAgentExecutor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtools\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtools\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m \u001B[0magent_executor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"input\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"吴京的老婆的主持过哪些节目\"\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    162\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 164\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    165\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    152\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m             outputs = (\n\u001B[1;32m--> 154\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    155\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m                 \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m   1606\u001B[0m         \u001B[1;31m# We now enter the agent loop (until it returns something).\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1607\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_should_continue\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterations\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime_elapsed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1608\u001B[1;33m             next_step_output = self._take_next_step(\n\u001B[0m\u001B[0;32m   1609\u001B[0m                 \u001B[0mname_to_tool_map\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1610\u001B[0m                 \u001B[0mcolor_mapping\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_take_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1312\u001B[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001B[0;32m   1313\u001B[0m         return self._consume_next_step(\n\u001B[1;32m-> 1314\u001B[1;33m             [\n\u001B[0m\u001B[0;32m   1315\u001B[0m                 \u001B[0ma\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m                 for a in self._iter_next_step(\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1312\u001B[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001B[0;32m   1313\u001B[0m         return self._consume_next_step(\n\u001B[1;32m-> 1314\u001B[1;33m             [\n\u001B[0m\u001B[0;32m   1315\u001B[0m                 \u001B[0ma\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m                 for a in self._iter_next_step(\n",
      "\u001B[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py\u001B[0m in \u001B[0;36m_iter_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1351\u001B[0m                 \u001B[0mraise_error\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1352\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mraise_error\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1353\u001B[1;33m                 raise ValueError(\n\u001B[0m\u001B[0;32m   1354\u001B[0m                     \u001B[1;34m\"An output parsing error occurred. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1355\u001B[0m                     \u001B[1;34m\"In order to pass this error back to the agent and have it try \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output:  Yes.\nFollowup: 谁是吴京的老婆?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilyAnswer\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_self_ask_with_search_agent\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# 使用SerpAPI搜索工具，搜索引擎替换为Baidu（默认是google）\n",
    "params = {\n",
    "  \"engine\": \"Baidu\"\n",
    "}\n",
    "search = SerpAPIWrapper(params=params)\n",
    "\n",
    "# self_ask_with_search_agent 只能传一个名为‘Intermediate Answer’的tool\n",
    "tools = [\n",
    "    # TavilyAnswer(max_results=1, name=\"Intermediate Answer\"),\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "print(\"=========prompt模板===========\")\n",
    "print(prompt.template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"吴京的老婆的主持过哪些节目\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 OpenAI Assistants\n",
    "\n",
    "支持OpenAI的Assistants API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\(10\\) 减去 \\(4\\) 的差的 \\(2.3\\) 次方是约 \\(61.624\\)。\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\n",
    "    name=\"langchain assistant\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-1106-preview\",\n",
    ")\n",
    "output = interpreter_assistant.invoke({\"content\": \"10减4的差的2.3次方是多少\"})\n",
    "\n",
    "print(output[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "<ol>\n",
    "<li>ReAct 是比较常用的 智能体</li>\n",
    "<li>SelfAskWithSearch 更适合需要层层推理的场景（例如知识图谱）</li>\n",
    "<li>OpenAI Assistants 不是万能的，后面课程中我们会专门讲 Agent 的实现</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、LangServe\n",
    "\n",
    "LangServe 用于将 Chain 或者 Runnable 部署成一个 REST API 服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 LangServe\n",
    "!pip install \"langserve[all]\"\n",
    "\n",
    "# 也可以只安装一端\n",
    "# !pip install \"langserve[client]\"\n",
    "# !pip install \"langserve[server]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1、Server端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"讲一个关于{topic}的笑话\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8080)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2、Client端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "joke_chain = RemoteRunnable(\"http://localhost:8080/joke/\")\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"小明\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "1. LangChain 随着版本迭代可用性有明显提升\n",
    "2. 使用 LangChain 要避开存在大量代码内 Prompt 的模块\n",
    "3. 它的内置基础工具，建议充分测试效果后再决定是否使用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}