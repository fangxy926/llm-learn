{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1、Semantic Kernel\n",
    "\n",
    "「 Semantic Kernel (SK) is a lightweight SDK that lets you easily mix conventional programming languages with the latest in Large Language Model (LLM) AI \"prompts\" with templating, chaining, and planning capabilities out-of-the-box. 」\n",
    "\n",
    "1. Semantic Kernel 是微软研发的一个开源的，面向大模型的开发框架（SDK）；\n",
    "2. 它支持你用不同开发语言（C#/Python/Java）基于 OpenAI API/Azure OpenAI API/Huggingface 开发大模型应用；\n",
    "3. 它封装了一系列开箱即用的工具，包括：提示词模板、链式调用、规划能力等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1、 大语言模型开发框架的价值是什么？\n",
    "所有开发框架（SDK）的核心价值，都是降低开发、维护成本。\n",
    "\n",
    "大语言模型开发框架的价值，是让开发者可以更方便地开发基于大语言模型的应用。主要提供三类帮助：\n",
    "\n",
    "1. 第三方能力抽象。比如 LLM、向量数据库、搜索引擎等\n",
    "2. 常用工具、方案封装\n",
    "3. 底层实现封装。比如流式接口、超时重连、异步与并行等\n",
    "\n",
    "举些通俗的例子：\n",
    "\n",
    "- 与外部功能解依赖\n",
    "  - 比如可以随意更换 LLM 而不用大量重构代码\n",
    "  - 更换三方工具也同理\n",
    "- 经常变的部分要在外部维护而不是放在代码里\n",
    "  - 比如 Prompt 模板\n",
    "- 各种环境下都适用\n",
    "  - 比如线程安全\n",
    "- 方便调试和测试\n",
    "  - 至少要能感觉到用了比不用方便吧\n",
    "  - 合法的输入不会引发框架内部的报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2、SK 的开发进展\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "~~Semantic Kernel 现在还是未正式发版状态。1.0.0 版预计今年底发布。~~\n",
    "\n",
    "1. 开发进展见项目介绍：https://github.com/microsoft/semantic-kernel \n",
    "2. 目前仅支持C#/Python/Java：https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages\n",
    "3. 不同版本支持的特性：https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages#available-features-in-each-sdk\n",
    "4. 文档：https://learn.microsoft.com/en-us/semantic-kernel/overview/\n",
    "5. 更多生态：https://github.com/geffzhang/awesome-semantickernel\n",
    "\n",
    "不同语言之间的概念都是相通的。本课程以 Python 版为例。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3、SK 的生态位\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "与 LangChain 完全重叠。微软将此技术栈命名为 Copilot Stack。\n",
    "\n",
    "<img src=\"copilot-stack.png\" alt=\"SK 的生态位\" width=\"400\"/>\n",
    "\n",
    "解释：\n",
    "\n",
    "- Plugin extensibility: 插件扩展\n",
    "- Copilots: AI 助手（副驾驶），例如 GitHub Copilot、Office 365 Copilot、Windows Copilot\n",
    "- AI orchestration: AI 编排，SK 就在这里\n",
    "- Foundation models: 基础大模型，例如 GPT-4\n",
    "- AI infrastructure: AI 基础设施，例如 PyTorch、GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 怎么理解这个 **AI 编排**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SK 是个野心勃勃的项目，它希望：\n",
    "\n",
    "1. 让开发者更容易的把 LLM 的能力集成到应用中，像调用函数一样简单\n",
    "2. 让开发者开发的 LLM 能力与应用解耦，高度可复用\n",
    "3. 让开发者能与微软的整个 Copilot 生态紧密结合，互相提供养料\n",
    "\n",
    "请带着这个视角，逐步体会后面所讲的知识。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4、SK 基础架构\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"mind-and-body-of-semantic-kernel.png\" alt=\"SK 的架构\" width=\"400\"/>\n",
    "\n",
    "解释：\n",
    "\n",
    "- Models and Memory: 类比为大脑\n",
    "- Connectors: 用来连接各种外部服务，类似驱动程序\n",
    "- Plugins: 用来连接内部技能\n",
    "- Triggers and actions: 外部系统的触发器和动作，类比为四肢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "！！！现阶段我们就这样理解ok\n",
    "    \n",
    "**类比：** Semantic Kernel 用 **Kernel** 命名，是因为它确实像个操作系统 kernel，做核心资源调配，各种资源都可以挂在它上。\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2、Hello SK！\n",
    "\n",
    "### 2.1 环境搭建\n",
    "\n",
    "1. 需要python版本>=3.10, <3.13（文档有说明：https://pypi.org/project/semantic-kernel/ ）\n",
    "\n",
    "2. 安装 SK 包：`pip install semantic-kernel==1.3.0`\n",
    "\n",
    "3. 如果安装了新环境中没有jupyter notebook，执行这条命令安装：`pip install notebook`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 简单示例\n",
    "\n",
    "第一段代码是初始化。后面所有代码都要在执行过这段代码后，才能执行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# 加载 .env 到环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 创建Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# 添加OpenAIChatCompletion服务\n",
    "service_id=\"default\"\n",
    "kernel.add_service(\n",
    "    OpenAIChatCompletion(\n",
    "        ai_model_id=\"gpt-3.5-turbo\",\n",
    "        service_id=service_id,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讲一个笑话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为了打破沉默，程序员对世界说：\"Hello world！\"世界回答说：\"你好，我是世界。\"程序员突然想起来忘了加分号，于是他说：\"不好意思，我忘了加分号。\"世界回答说：\"没关系，我喜欢你这种简单直接的方式。\"笑话的主题是程序员经典的Hello world程序，通过这个简单的问候，展示了程序员与世界的奇妙互动。\n"
     ]
    }
   ],
   "source": [
    "# 参数用{{}}标识\n",
    "joke_prompt = \"给我讲个关于{{$input}}的笑话吧\"\n",
    "\n",
    "# 定义function\n",
    "joke_function = kernel.add_function(\n",
    "    plugin_name=\"joke_function\",\n",
    "    function_name=\"joke_function\",\n",
    "    prompt=joke_prompt\n",
    ")\n",
    "\n",
    "# 运行 function 看结果\n",
    "async def run_function():\n",
    "    return await kernel.invoke(\n",
    "        joke_function,\n",
    "        input=\"Hello world\" # 只有一个入参，可以直接指定\n",
    "    )\n",
    "\n",
    "# 注意这里直接使用 await 如果你在本地运行请执行：asyncio.run(run_function())\n",
    "result = await run_function()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、Kernel & Service\n",
    "\n",
    "<img src=\"kernel.png\" alt=\"Kernel\" width=\"800\"/>\n",
    "\n",
    "### 3.1、Kernel能干什么？\n",
    "\n",
    "1. Select the best AI service to run the prompt. 选择最佳的service 来运行prompt\n",
    "\n",
    "2. Build the prompt using the provided prompt template.使用提供的提示模板构建提示\n",
    "\n",
    "3. Send the prompt to the AI service. 将提示发送到 AI 服务\n",
    "\n",
    "4. Receive and parse the response. 接收并解析响应\n",
    "\n",
    "5. Return the response from the LLM to your application. 将LLM的响应返回给你的应用\n",
    "\n",
    "\n",
    "### 3.2 Service是什么？\n",
    "\n",
    "1. Service 包括运行应用程序所需的 AI 服务（如chat completion）和其他服务（如日志记录和HTTP请求服务）。\n",
    "\n",
    "2. 定义service id是为了能在sk中唯一标识一个服务\n",
    "\n",
    "<img src=\"services.png\" alt=\"Service\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4、Plugins\n",
    "简单说，plugin 就是一组函数的集合。它可以包含两种函数：\n",
    "\n",
    "- Semantic Functions - 语义函数，本质是 Prompt Engineering\n",
    "- Native Functions - 原生函数，类似 OpenAI 的 Function Calling\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>注意：</b>Plugins 最初命名为 Skills，后来改为 Plugins。但是无论文档还是代码，可能还有大量的「Skill」遗留，预计到 1.0.0 发布才能清理干净。见到后，就知道两者是一回事就好。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK 内置了若干好用的 plugin\n",
    "\n",
    "加载方法：\n",
    "\n",
    "```python\n",
    "from semantic_kernel.core_plugins import xxx\n",
    "```\n",
    "\n",
    "它们是：\n",
    "\n",
    "- [`ConversationSummaryPlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/conversation_summary_plugin.py) - 生成对话的摘要\n",
    "- [`HttpPlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/http_plugin.py) - 发出 HTTP 请求，支持 GET、POST、PUT 和 DELETE\n",
    "- [`MathPlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/math_plugin.py) - 加法和减法计算\n",
    "- [`TextMemoryPlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/text_memory_plugin.py) - 保存文本到 memory 中，可以对其做向量检索\n",
    "- [`TextPlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/text_plugin.py) - 把文本全部转为大写或小写，去掉头尾的空格（trim）\n",
    "- [`TimePlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/time_plugin.py) - 获取当前时间及用多种格式获取时间参数\n",
    "- [`WaitPlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/wait_plugin.py) - 等待指定的时间\n",
    "- [`WebSearchEnginePlugin`](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/web_search_engine_plugin.py) - 在互联网上搜索给定的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.1、Semantic Functions\n",
    "\n",
    "Semantic Functions 是纯用数据（Prompt + 配置文件）定义的，不需要编写任何代码。所以它与编程语言无关，可以被任何编程语言调用。\n",
    "\n",
    "一个典型的 semantic function 包含两个文件：\n",
    "\n",
    "- skprompt.txt: 存放 prompt，可以包含参数，还可以调用其它函数\n",
    "- config.json: 存放配置，包括函数功能，参数的数据类型，以及调用大模型时的参数\n",
    "\n",
    "举例：根据用户的自然语言指示，生成 Linux 命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.1、skprompt.txt\n",
    "\n",
    "- 参数用{{}}标识"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "将用户的指令转换成 Linux 命令\n",
    "只输出命令本身，不要分析，不要评论。\n",
    "\n",
    "{{$input}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.2、config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"schema\": 1,\n",
    "    \"type\": \"completion\",\n",
    "    \"description\": \"将用户指令转为Linux指令\",\n",
    "    \"execution_settings\": {\n",
    "        \"default\": {\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0\n",
    "        }\n",
    "    },\n",
    "    \"input_variables\": [\n",
    "        {\n",
    "            \"name\": \"input\",\n",
    "            \"description\": \"用户的指令\",\n",
    "            \"default\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "说明：\n",
    "\n",
    "- `type` 只有 `\"completion\"` 和 `\"embedding\"` 两种\n",
    "- 视频中的写法和我这里的写法有点不一样，我这里参考最新官方文档写的，视频里的好像也能用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上面两个文件都在 [sk_samples/SamplePlugin/GenerateCommand](sk_samples/SamplePlugin/GenerateCommand/) 目录下。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1.3、调用 Semantic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date -s \"2023-04-01\"\n"
     ]
    }
   ],
   "source": [
    "# 加载 semantic function。注意目录结构\n",
    "plugin = kernel.add_plugin(parent_directory=\"./sk_samples/\", plugin_name=\"SamplePlugin\")\n",
    "\n",
    "# 运行 function 看结果\n",
    "async def run_function():\n",
    "    # 运行 GenerateCommand plugin 看结果\n",
    "    return await kernel.invoke(\n",
    "        plugin[\"GenerateCommand\"],\n",
    "        input=\"将系统日期设为2023-04-01\"\n",
    "    )\n",
    "\n",
    "result = await run_function()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2、Native Functions\n",
    "\n",
    "用编程语言写的函数，如果用 SK 的 Native Function 方式定义，就能纳入到 SK 的编排体系，可以被 Planner、其它 plugin 调用。\n",
    "\n",
    "下面，写一个过滤有害 Linux 命令的函数，和 GenerateCommand 组合使用。\n",
    "\n",
    "这个函数名是 `verify`。如果输入的命令不在规定范围内，就返回 `非法`，否则返回 `合法`。\n",
    "\n",
    "它可以放到目录结构中，在 [sk_samples/SamplePlugin/SamplePlugin.py](sk_samples/SamplePlugin/SamplePlugin.py) 里加入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class CommandVerifier:\n",
    "    # 1.3.0 版本中使用 kernel_function\n",
    "    @kernel_function(\n",
    "        description=\"检查命令是否合法\",\n",
    "        name=\"verifyCommand\",\n",
    "    )\n",
    "    def verify(self, command: str) -> str:\n",
    "        if \">\" in command:\n",
    "            return \"非法\"\n",
    "        parts = command.replace(';', '|').split('|')\n",
    "        for cmd in parts:\n",
    "            name = cmd.split(\" \")[0]\n",
    "            if name not in [\"ls\",\"cat\",\"head\",\"tail\",\"echo\"]:\n",
    "                return \"非法\"\n",
    "        return \"合法\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>新版本中使用@kernel_function()标识本地方法，而不是视频中使用的@sk_function()\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非法\n"
     ]
    }
   ],
   "source": [
    "# 加载 native function\n",
    "verify_plugin = kernel.add_plugin(CommandVerifier(), \"CommandVerifier\")\n",
    "\n",
    "# 看结果\n",
    "result = await kernel.invoke(\n",
    "    verify_plugin[\"verifyCommand\"],\n",
    "    command='date -s \"2023-04-01\"'\n",
    "    # command='ls -l'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>在 SK 中，Semantic Function 和 Native Function 被 Kernel 平等对待。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.3、用 KernelArguments 实现多参数 Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "如果 Function 都只有一个参数，那么只要把参数定义为 `{{$input}}`，就可以按前面的例子来使用，比较直观。`{{$input}}`会默认被赋值。\n",
    "\n",
    "多参数时，就不能用默认机制了，需要定义 `KernelArguments` 类型的变量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3.1、多参数 Semantic Function 的写法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "农夫对狼说：“你想吃我吗？不如我们一起去捕捉那只绵羊，我会请你吃烤肉！”\n"
     ]
    }
   ],
   "source": [
    "# Prompt 模板\n",
    "sk_prompt = \"\"\"\n",
    "讲一个{{$topic1}}和{{$topic2}}的一句话笑话\n",
    "\"\"\"\n",
    "\n",
    "# 创建 Semantic Function\n",
    "joke_function = kernel.add_function(\n",
    "    function_name=\"joke_function\",\n",
    "    plugin_name=\"joke_function\",\n",
    "    prompt=sk_prompt\n",
    ")\n",
    "\n",
    "# 创建 KernelArguments，并变量赋值\n",
    "arguments = KernelArguments(topic1=\"农夫\", topic2=\"狼\")\n",
    "\n",
    "# 看结果\n",
    "result = await kernel.invoke(\n",
    "    function = joke_function,\n",
    "    arguments = arguments\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3.2、多参数 Native Function 的写法\n",
    "\n",
    "使用`Annotated`给入参添加说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class Math:\n",
    "    @kernel_function(\n",
    "        description=\"加法\",\n",
    "        name=\"add\",\n",
    "    )\n",
    "    def add(self, \n",
    "            number1: Annotated[float, \"被加数\"],\n",
    "            number2: Annotated[float, \"加数\"]\n",
    "            ) -> str:\n",
    "        return number1 + number2\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"减法\",\n",
    "        name=\"minus\",\n",
    "    )\n",
    "    \n",
    "    def minus(self, \n",
    "            number1: Annotated[float, \"被减数\"],\n",
    "            number2: Annotated[float, \"减数\"]\n",
    "            ) -> str:\n",
    "        return number1 - number2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加法计算结果：66560.0\n",
      "减法计算结果：-64512.0\n"
     ]
    }
   ],
   "source": [
    "# 加载 native function\n",
    "math_plugin = kernel.add_plugin(Math(), \"Math\")\n",
    "\n",
    "# 创建 KernelArguments，并变量赋值\n",
    "arguments = KernelArguments(number1=1024, number2=65536)\n",
    "\n",
    "# 看结果\n",
    "result = await kernel.invoke(\n",
    "    function = math_plugin[\"add\"],\n",
    "    arguments = arguments\n",
    ")\n",
    "print(f\"加法计算结果：{result}\")\n",
    "\n",
    "result = await kernel.invoke(\n",
    "    function = math_plugin[\"minus\"],\n",
    "    arguments = arguments\n",
    ")\n",
    "print(f\"减法计算结果：{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.4、函数的嵌套调用\n",
    "\n",
    "### 4.4.1、Semantic Function 嵌套调用\n",
    "\n",
    "SK 允许在 Prompt 模板中直接调用一个函数，形如\n",
    "```\n",
    "{{NestedSample.VerifyCommand $input}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非法，删除当前目录是一个危险且不可逆的操作\n"
     ]
    }
   ],
   "source": [
    "# 加载 semantic function。注意目录结构\n",
    "nested_sample = kernel.add_plugin(parent_directory=\"./sk_samples/\", plugin_name=\"NestedSample\")\n",
    "\n",
    "# 看结果\n",
    "result = await kernel.invoke(\n",
    "    nested_sample[\"GenerateCommand\"],\n",
    "    input=\"删除当前目录\",\n",
    "    # input=\"显示 example.txt 文件的内容\",\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.4.2、Native Function 嵌套调用（选）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**注意：** Native Function 的嵌套调用，本质上就是函数嵌套。官方给的写法是在 Kernel 的设计思想下的实现，观感上非常晦涩。\n",
    "\n",
    "实际开发中，可以根据个人对 SK 内核与设计理念的理解，自行选择使用以下写法，或使用普通的函数调用的写法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5、Memory\n",
    "\n",
    "Semantic Kernel将embedding的功能封装到了Memory中，用来存储上下文信息，就好像电脑的内存一样，而LLM就像是CPU一样，我们所需要做的就是从内存中取出相关的信息交给CPU处理就好了。\n",
    "\n",
    "使用流程：\n",
    "\n",
    "1. 用 `kernel.add_service()` 添加一个文本向量生成服务\n",
    "2. 注册一个`VolatileMemoryStore`，可以是内存、文件、向量数据库等\n",
    "3. 用 `memory.save_information()` 保存信息到`VolatileMemoryStore`\n",
    "4. 用 `memory.search()` 搜索信息\n",
    "\n",
    "使用 ChatALL 的 README.md 做数据，使用内存作为 memory store，我们演示下基于文档对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.1、初始化 Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "import os\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "\n",
    "\n",
    "# 加载 .env 到环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 创建 semantic kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "chat_service_id=\"default\"\n",
    "\n",
    "oai_chat_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-3.5-turbo\",\n",
    "    service_id=chat_service_id,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "embedding_gen = OpenAITextEmbedding(\n",
    "    ai_model_id=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 添加OpenAIChatCompletion服务\n",
    "kernel.add_service(oai_chat_service)\n",
    "\n",
    "# 添加 embedding 服务\n",
    "kernel.add_service(embedding_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.2、文本向量化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div align=\"center\">\n",
      "   <img src=\"src/assets/logo-cover.png\" width=256></img>\n",
      "   <p><strong>同时与所有 AI 机器人聊天，找到最佳答案</strong></p>\n",
      "\n",
      "[Deutsch](README_DE-DE.md) | [English](README.md) | [Español](README_ES-ES.md) | [Français](README_FR-FR.md) | [Italian](README_IT-IT.md) | [日本語](README_JA-JP.md) | [한국어](README_KO-KR.\n",
      "md) | [Русский](README_RU-RU.md) | [Tiếng Việt](README_VI-VN.md) | 简体中文\n",
      "\n",
      "</div>\n",
      "\n",
      "## 屏幕截图\n",
      "\n",
      "![Screenshot](screenshots/screenshot-1.png?raw=true)\n",
      "\n",
      "## 功能\n",
      "\n",
      "基于大型语言模型（LLMs）的 AI 机器人非常神奇。然而，它们的行为可能是随机的，不同的机器人在不同的任务上表现也有差异。如果你想获得最佳体验，不要一个一个尝试。ChatALL（中文名：齐叨）可以把一条指令同时发给多个 AI，帮助您发现最好的回答。你需要做的只是[下载、安装](https://github.\n",
      "com/sunner/ChatALL/releases)\n",
      "和提问。\n",
      "\n",
      "### 这是你吗？\n",
      "\n",
      "ChatALL 的典型用户是：\n",
      "\n",
      "- 🤠**大模型重度玩家**，希望从大模型找到最好的答案，或者最好的创作\n",
      "- 🤓**大模型研究者**，直观比较各种大模型在不同领域的优劣\n",
      "- 😎**大模型应用开发者**，快速调试 prompt，寻找表现最佳的基础模型\n",
      "\n",
      "### 支持的 AI\n",
      "\n",
      "| AI 机器人\n",
      "| 网页访问 | API      | 说明                                     |\n",
      "| ------------------------------------------------------------------------------ | -------- | -------- | ---------------------------------------- |\n",
      "| [ChatGPT]\n",
      "(https:\n",
      "//chat.\n",
      "openai.com)                                             | 支持     | 支持     | 包含 Web Browsing、Azure OpenAI service  |\n",
      "| [Bing Chat](https://www.bing.com/new)                                          | 支持     | 无 API   | 不需要帐号                               |\n",
      "| [文心一言](https://yiyan.baidu.\n",
      "com/)                                           | 否       | 支持     |                                          |\n",
      "| [Bard](https://bard.google.com/)                                               | 支持     | 即将推出 |                                          |\n",
      "| [Poe](https://poe.\n",
      "com/)                                                        | 支持     | 即将推出 |                                          |\n",
      "| [MOSS](https://moss.\n",
      "fastnlp.top/)                                              | 支持     | 无 API   |                                          |\n",
      "| [通义千问](http://tongyi.aliyun.\n",
      "com/)                                          | 支持     | 即将推出 |                                          |\n",
      "| [得到学习助手](https://ai.dedao.cn/)                                           | 即将推出 | 无 API   |                                          |\n",
      "| [讯飞星火](http://xinghuo.xfyun.\n",
      "cn/)                                           | 支持     | 即将推出 |                                          |\n",
      "| [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.\n",
      "html)                     | 支持     | 无 API   | 不需要帐号                               |\n",
      "| [Vicuna 7B、13B 和 33B](https://lmsys.org/blog/2023-03-30-vicuna/)             | 支持     | 无 API   | 不需要帐号                               |\n",
      "| [ChatGLM2 6B 和 130B](https://chatglm.\n",
      "cn/blog)                                 | 支持     | 无 API   | 不需要帐号                               |\n",
      "| [Claude 2 和 Instant](https://www.anthropic.com/index/claude-2)                | 支持     | 无 API   | 不需要帐号                               |\n",
      "| [Gradio](httpps://gradio.\n",
      "app/)                                                 | 支持     | 无 API   | 用于 Hugging Face space 或自己部署的模型 |\n",
      "| [HuggingChat](https://huggingface.co/chat/)                                    | 支持     | 无 API   |\n",
      "| [天工](https://neice.\n",
      "tiangong.cn/)                                             | 支持     | 即将推出 |                                          |\n",
      "| [You](https://you.com/)                                                        | 支持     | 无 API   |                                          |\n",
      "| [Pi](https://pi.\n",
      "ai)                                                            | 支持     | 无 API   |                                          |\n",
      "| [360 智脑](https://ai.\n",
      "360.cn/)                                                 | 支持     | 无       |                                          |\n",
      "| [YouChat](https://you.\n",
      "com/)                                                    | 支持     | 无       |                                          |\n",
      "| [Open Assistant](https://open-assistant.io/)                                   | 支持     | 无       |                                          |\n",
      "| [Character.AI](https://character.\n",
      "ai/)                                          | 支持     | 无       |                                          |\n",
      "| [Llama 2 7B、13B 和 70B](https://ai.meta.com/llama/)                           | 支持     | 无 API   |                                          |\n",
      "| [Code Llama](https://ai.\n",
      "meta.com/blog/code-llama-large-language-model-coding/) | 支持     | 无 API   |                                          |\n",
      "| [WizardLM 13B 和 70B](https://github.\n",
      "com/nlpxucan/WizardLM)                    | 支持     | 无 API   |                                          |\n",
      "| [Falcon 180B](https://huggingface.co/tiiuae/falcon-180B-chat)                  | 支持     | 无 API   |                                          |\n",
      "| [Phind](https://www.\n",
      "phind.com/)                                                | 支持     | 无 API   |                                          |\n",
      "\n",
      "还会有更多。[到这里](https://github.\n",
      "com/sunner/ChatALL/labels/more%20LLMs)\n",
      "为你喜欢的 AI 投票吧。\n",
      "\n",
      "### 其他功能\n",
      "\n",
      "- 快问模式：不需要等待前面的请求完成，就可以发下一条指令\n",
      "- 对话历史保存在本地，保护你的隐私\n",
      "- 高亮喜欢的答案，删除不需要的答案\n",
      "- 随时启用/禁用任何机器人\n",
      "- 在一列、两列或三列视图之间切换\n",
      "- 自动更新到最新版\n",
      "- 夜间模式（由 @tanchekwei 贡献）\n",
      "- 快捷键。按 `Ctrl + /` 可以看到所有快捷键（由 @tanchekwei 贡献）\n",
      "- 多对话窗口（由 @tanchekwei 贡献）\n",
      "- 支持设置代理（由 @msaong 贡献）\n",
      "- 提示词管理（由 @tanchekwei 贡献）\n",
      "- 支持多语言（中文、英语、德语、法语、俄语、越南语、韩语、日语、西班牙语、意大利语）\n",
      "- 支持 Windows，macOS 和 Linux\n",
      "\n",
      "计划中：\n",
      "\n",
      "欢迎参与这些功能的开发。\n",
      "\n",
      "- [ ]\n",
      "把前端部署到 GitHub Pages\n",
      "\n",
      "## 预先需要\n",
      "\n",
      "ChatALL 是一个客户端，而不是代理。因此，您必须：\n",
      "\n",
      "1.\n",
      "拥有可以访问这些 AI 的帐号，或 API token。\n",
      "2. 与 AI 网站有可靠的网络连接。\n",
      "\n",
      "## 下载 / 安装\n",
      "\n",
      "从 https://github.com/sunner/ChatALL/releases 下载\n",
      "\n",
      "### Windows 系统\n",
      "\n",
      "直接下载 \\*-win.exe 安装文件并运行之。\n",
      "\n",
      "### macOS 系统\n",
      "\n",
      "对于苹果硅芯片 Mac（M1，M2 CPU），请下载 \\*-mac-arm64.\n",
      "dmg 文件。\n",
      "\n",
      "对于其他 Mac，下载 \\*-mac-x64.dmg 文件。\n",
      "\n",
      "如果你在使用 [Homebrew](https://brew.sh/)，可以使用以下命令安装：\n",
      "\n",
      "```bash\n",
      "brew install --cask chatall\n",
      "```\n",
      "\n",
      "### Linux 系统\n",
      "\n",
      "下载 .AppImage 文件，将其设置为可执行，然后双击就可运行。\n",
      "\n",
      "## 问题解决\n",
      "\n",
      "使用中如果遇到问题，可以尝试如下方法解决：\n",
      "\n",
      "1.\n",
      "**刷新** - 按下 `Ctrl + R` 或 `Cmd + R`。\n",
      "2. **重启** - 退出 ChatALL，再重新运行。\n",
      "3. **重新登录** - 点击右上角的设置按钮，在弹出的窗口中点击对应的登入/登出链接，重新登录网站。\n",
      "4.\n",
      "**创建新对话** - 创建一个新对话，再发送提示词。\n",
      "\n",
      "如果以上方法都不行，那么可以尝试**重置 ChatALL**。注意，这将丢失你所有的设置和消息历史。\n",
      "\n",
      "删除下面的目录可以重置 ChatALL：\n",
      "\n",
      "- Windows: `C:\\Users\\<user>\\AppData\\Roaming\\ChatALL\\`\n",
      "- Linux: `/home/<user>/.config/ChatALL/`\n",
      "- macOS: `/Users/<user>/Library/Application Support/ChatALL/`\n",
      "\n",
      "如果问题依旧，请[提交一个 issue](https://github.\n",
      "com/sunner/ChatALL/issues)。\n",
      "\n",
      "## 给开发者\n",
      "\n",
      "### 贡献新的 AI 机器人\n",
      "\n",
      "[这份文档](https://github.\n",
      "com/sunner/ChatALL/wiki/%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84-AI-%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA)能提供一些帮助。\n",
      "\n",
      "### Run\n",
      "\n",
      "```bash\n",
      "npm install\n",
      "npm run electron:serve\n",
      "```\n",
      "\n",
      "### Build\n",
      "\n",
      "为您当前的平台构建：\n",
      "\n",
      "```bash\n",
      "npm run electron:build\n",
      "```\n",
      "\n",
      "为所有平台构建：\n",
      "\n",
      "```bash\n",
      "npm run electron:build -- -wml --x64 --arm64\n",
      "```\n",
      "\n",
      "## 致谢\n",
      "\n",
      "### 贡献者\n",
      "\n",
      "<a href=\"https://github.\n",
      "com/sunner/ChatALL/graphs/contributors\">\n",
      "  <img src=\"https://contrib.rocks/image?repo=sunner/ChatALL\" />\n",
      "</a>\n",
      "\n",
      "### 其它\n",
      "\n",
      "- GPT-4 贡献了大部分代码\n",
      "- ChatGPT，Bing Chat 和 Google 提供了许多解决方案（排名分先后）\n",
      "- 受 [ChatHub](https://github.com/chathub-dev/chathub) 启发。致敬！\n",
      "\n",
      "## 赞助\n",
      "\n",
      "如果您喜欢这个项目，可以通过以下方式支持：\n",
      "\n",
      "<img src=\"https://sunner.cn/sponsor/alipay.jpg\" width=\"200\" />\n",
      "<img src=\"https://sunner.cn/sponsor/wepay.jpg\" width=\"200\" />\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.text import split_markdown_lines\n",
    "\n",
    "# 使用VolatileMemoryStore存放memory数据\n",
    "memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_gen)\n",
    "\n",
    "# 读取文件内容\n",
    "with open('ChatALL.md', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 将文件内容分片，单片最大 100 token（注意：SK 的 text split 功能目前对中文支持不如对英文支持得好）\n",
    "lines = split_markdown_lines(content, 100)\n",
    "\n",
    "# 将分片后的内容，存入内存\n",
    "for index, line in enumerate(lines):\n",
    "    print(line)\n",
    "    await memory.save_information(collection=\"chatall\", id=index, text=line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.3、向量搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拥有可以访问这些 AI 的帐号，或 API token。\n",
      "2. 与 AI 网站有可靠的网络连接。\n",
      "\n",
      "## 下载 / 安装\n",
      "\n",
      "从 https://github.com/sunner/ChatALL/releases 下载\n",
      "\n",
      "### Windows 系统\n",
      "\n",
      "直接下载 \\*-win.exe 安装文件并运行之。\n",
      "\n",
      "### macOS 系统\n",
      "\n",
      "对于苹果硅芯片 Mac（M1，M2 CPU），请下载 \\*-mac-arm64.\n"
     ]
    }
   ],
   "source": [
    "result = await memory.search(collection=\"chatall\", query=\"ChatALL怎么下载？\")\n",
    "print(result[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.4、现在用函数嵌套做一个简单的 RAG\n",
    "\n",
    "例：基于 ChatALL 的说明文档，做问答\n",
    "\n",
    "在自定义的 Semantic Function 中，嵌套调用内置的 `TextMemoryPlugin`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从 https://github.com/sunner/ChatALL/releases 下载对应系统的安装文件。对于Windows系统，下载 \\*-win.exe 文件并运行；对于苹果硅芯片Mac，下载 \\*-mac-arm64 文件。\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# 导入内置的 `TextMemoryPlugin`。主要使用它的 `recall()`\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "\n",
    "# 直接在代码里创建 semantic function。真实工程不建议这么做\n",
    "# 里面调用了 `recall()`\n",
    "sk_prompt = \"\"\"\n",
    "基于下面的背景信息回答问题。如果背景信息为空，或者和问题不相关，请回答\"我不知道\"。\n",
    "\n",
    "[背景信息开始]\n",
    "{{recall $input}}\n",
    "[背景信息结束]\n",
    "\n",
    "问题：{{$input}}\n",
    "回答：\n",
    "\"\"\"\n",
    "ask = kernel.add_function(\n",
    "    function_name=\"chat_with_memory\",\n",
    "    plugin_name=\"chat\",\n",
    "    prompt = sk_prompt\n",
    ")\n",
    "\n",
    "# 提问\n",
    "result = await kernel.invoke(\n",
    "    ask,\n",
    "    KernelArguments(\n",
    "        collection = \"chatall\",\n",
    "        relevance = 0.8,\n",
    "        input=\"ChatALL 怎么下载？\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.5、连接其它 VectorDB\n",
    "\n",
    "Semantic Kernel 目前已与很多主流的向量数据库做了适配\n",
    "\n",
    "具体参考：https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6、Planner\n",
    "\n",
    "SK 的 Planner 目的是 Agent 开发。只封装了几个基本形式，把更多的探索留给了开发者。\n",
    "\n",
    "### 6.1、什么是智能体（Agent）\n",
    "\n",
    "将大语言模型作为一个推理引擎。给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。\n",
    "\n",
    "这个多步骤的规划过程，就由 **Planner** 完成。\n",
    "\n",
    "<img src=\"agent-overview.png\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "### 6.2、SK Python 提供了~~四~~两种 Planner：\n",
    "\n",
    "1. `SequentialPlanner`\n",
    "   - 制定包含一系列步骤的计划，这些步骤通过自定义生成的输入和输出变量相互连接\n",
    "   - 官方例程：https://github.com/microsoft/semantic-kernel/blob/main/python/samples/concepts/planners/sequential_planner.py\n",
    "2. `FunctionCallingStepwisePlanner`\n",
    "   - 类似 OpenAI Function Calling，从 kernel 中已注册的所有 plugin 中找到一个该执行的函数\n",
    "   - 官方例程：https://github.com/microsoft/semantic-kernel/blob/main/python/samples/concepts/planners/openai_function_calling_stepwise_planner.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6.3、SequentialPlaner案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the current day of the week : {'execution_settings': None}\n",
      "Convert a string to uppercase. : {'execution_settings': None}\n",
      "Expected Answer:\n",
      "MONDAY\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.core_plugins import MathPlugin, TextPlugin, TimePlugin\n",
    "from semantic_kernel.planners import SequentialPlanner\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "async def main():\n",
    "    kernel = Kernel()\n",
    "\n",
    "    service_id = \"gpt-3.5\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id, \n",
    "            ai_model_id=\"gpt-3.5-turbo\",\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 添加plugin\n",
    "    kernel.add_plugins({\"math\": MathPlugin(), \"time\": TimePlugin(), \"text\": TextPlugin()})\n",
    "\n",
    "    # 创建 sequential planner.\n",
    "    planner = SequentialPlanner(service_id=service_id, kernel=kernel)\n",
    "\n",
    "    # 题问\n",
    "    ask = \"What day of the week is today, all uppercase?\"\n",
    "    \n",
    "    # 创建 plan\n",
    "    plan = await planner.create_plan(goal=ask)\n",
    "\n",
    "    # 执行 plan\n",
    "    result = await plan.invoke(kernel=kernel)\n",
    "\n",
    "    # 打印 plan中的每一步\n",
    "    for step in plan._steps:\n",
    "        print(step.description, \":\", step._state.__dict__)\n",
    "\n",
    "    print(\"Expected Answer:\")\n",
    "    print(result)\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    SUNDAY\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4、Planner的优势\n",
    "\n",
    "想想自己写Funcation Calling的过程：\n",
    "\n",
    "1、编写Json格式的函数定义文件\n",
    "\n",
    "2、调用LLM，传入上下文问和函数定义文件\n",
    "\n",
    "3、解析LLM的响应以确定是否要调用函数\n",
    "\n",
    "4、如果有函数调用，需要解析参数和在本地调用函数\n",
    "\n",
    "5、返回函数调用结果，再传给LLM\n",
    "\n",
    "6、重复步骤2-5\n",
    "\n",
    "使用SK的Planner可以自动执行循环调用过程，让使用者更加专注于构建解决用户需求所需的插件。\n",
    "\n",
    "为什么可以做到？\n",
    "\n",
    "> Today's LLM models are capable of iteratively calling functions to solve a user's need. This is accomplished by creating a feedback loop where the AI can call a function, check the result, and then decide what to do next.\n",
    "\n",
    "今天的LLM模型能够迭代调用函数来解决用户的需求。这是通过创建一个反馈循环来实现的，其中人工智能可以调用函数、检查结果，然后决定下一步做什么。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7、VS Code 插件\n",
    "\n",
    "这是个 VS Code 的插件，在 VS Code 里可以直接创建和调试 Semantic Function。\n",
    "\n",
    "安装地址：https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 总结\n",
    "\n",
    "1. 我是否应该使用开发框架？\n",
    "2. 什么情况下选择 SK ？\n",
    "\n",
    "- 如果你经常需要替换不同 LLM 或有大量的 Prompt 调试需求，选择一个开发框架会让生活更容易\n",
    "- 如果你必须使用 C#/JAVA 技术栈，SK 可能是目前唯一的选择\n",
    "- 如果你用 Python 技术栈，可以对比一下 LangChain 再做取舍（下节课细讲）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
